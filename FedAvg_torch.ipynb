{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive/ColabTemp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import shutil\n",
    "import h5py\n",
    "from FEMNIST_by_write import get_client_datasets, get_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducing code\n",
    "mySeed = 42\n",
    "random.seed(mySeed)  # Python random module.\n",
    "np.random.seed(mySeed)  # Numpy module.\n",
    "torch.manual_seed(mySeed)\n",
    "torch.cuda.manual_seed(mySeed)\n",
    "torch.cuda.manual_seed_all(mySeed)  # if you are using multi-GPU.\n",
    "\n",
    "dataloader_generator = torch.Generator()\n",
    "dataloader_generator.manual_seed(mySeed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_range(mini, maxi):\n",
    "    \"\"\"Return function handle of an argument type function for \n",
    "       ArgumentParser checking a float range: mini <= arg <= maxi\n",
    "         mini - minimum acceptable argument\n",
    "         maxi - maximum acceptable argument\"\"\"\n",
    "\n",
    "    # Define the function with default arguments\n",
    "    def float_range_checker(arg):\n",
    "        \"\"\"New Type function for argparse - a float within predefined range.\"\"\"\n",
    "\n",
    "        try:\n",
    "            f = float(arg)\n",
    "        except ValueError:    \n",
    "            raise argparse.ArgumentTypeError(\"must be a floating point number\")\n",
    "        if f < mini or f > maxi:\n",
    "            raise argparse.ArgumentTypeError(\"must be in range [\" + str(mini) + \" .. \" + str(maxi)+\"]\")\n",
    "        return f\n",
    "\n",
    "    # Return function handle to checking function\n",
    "    return float_range_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters (Auto)\n",
    "parser = argparse.ArgumentParser(prog='FedSwap',\n",
    "                                 description=\"FedSwap: Converging Federated Learning Faster\",\n",
    "                                 epilog=\"Written by Ali Bozorgzad based on FedSwap paper\")\n",
    "\n",
    "parser.add_argument(\"--dataset\", \"-d\", dest=\"dataset_name\", type=str, default=\"CIFAR10\",\n",
    "                    help=\"dataset name\", choices=['MNIST', 'CIFAR10', 'CINIC10', 'FEMNIST', 'FEMNISTwriter'])\n",
    "parser.add_argument(\"--NN_type\", \"-n\", dest=\"neural_network_type\", type=str, default=\"Conv\",\n",
    "                    help=\"neural network type\", choices=['MLP', 'Conv'])\n",
    "parser.add_argument(\"--num_clients\", \"-c\", dest=\"num_clients\", type=int, default=\"10\",\n",
    "                    help=\"number of clients, except for 'FEMNISTwriter', cause it fixed\")\n",
    "parser.add_argument(\"--batch_size\", \"-b\", dest=\"batch_size\", type=int, default=\"64\",\n",
    "                    help=\"batch size\")\n",
    "parser.add_argument(\"--total_steps\", \"-t\", dest=\"total_steps\", type=int, default=\"301\",\n",
    "                    help=\"total steps\")\n",
    "parser.add_argument(\"--client_per\", \"-p\", dest=\"client_select_percentage\", type=float_range(1e-2, 1), default=\"1\",\n",
    "                    help=\"client selection percentage, between [1e-2...1] 1 is 100%%\")\n",
    "parser.add_argument(\"--clients_data_distribution\", \"-u\", dest=\"clients_data_distribution\", type=str, default=\"equal\",\n",
    "                    help=\"how distribute train data between clients\", choices=['equal', 'random'])\n",
    "parser.add_argument(\"--random_split\", \"-r\", dest=\"data_random_split\", type=int, default=\"1\",\n",
    "                    help=\"data random split between clients\", choices=[0, 1])\n",
    "parser.add_argument(\"--learning_rate\", \"-l\", dest=\"learning_rate\", type=float, default=\"1e-3\",\n",
    "                    help=\"learning rate\")\n",
    "parser.add_argument(\"--client_epochs\", \"-e\", dest=\"client_epochs\", type=int, default=\"1\",\n",
    "                    help=\"client epochs\")\n",
    "parser.add_argument(\"--remain\", dest=\"remain\", type=float_range(1e-3, 1), default=\"1\",\n",
    "                    help=\"remain %% of dataset for running faster in test, between [1e-3...1] 1 is 100%%, except for 'FEMNISTwriter'\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "dataset_name = args.dataset_name\n",
    "neural_network_type = args.neural_network_type\n",
    "\n",
    "num_clients = args.num_clients\n",
    "batch_size = args.batch_size\n",
    "total_steps = args.total_steps\n",
    "client_select_percentage = args.client_select_percentage\n",
    "clients_data_distribution = args.clients_data_distribution\n",
    "data_random_split = args.data_random_split\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = args.client_epochs\n",
    "\n",
    "remain = args.remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters (Manual)\n",
    "dataset_name = \"FEMNISTwriter\" # 'MNIST' or 'CIFAR10' or 'CINIC10' or 'FEMNIST' or 'FEMNISTwriter'\n",
    "neural_network_type = \"Conv\" # 'MLP' or 'Conv'\n",
    "\n",
    "num_clients = 10 # except for 'FEMNISTwriter'\n",
    "batch_size = 64\n",
    "total_steps = 46\n",
    "client_select_percentage = 1\n",
    "clients_data_distribution = 1 # 'equal' or 'random'\n",
    "data_random_split = 1 # 0 or 1\n",
    "\n",
    "learning_rate = 1e-3\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = 1\n",
    "\n",
    "remain = 0.001 # Remove some data for running faster in test, except for 'FEMNISTwriter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_log_file_name: 'FA_FEMNISTwriter_Conv_3597c_64b_46s_1cp_1rs_0.001lr_1ce_step'\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "client_selects = None\n",
    "client_weights = None\n",
    "\n",
    "passed_steps = 0\n",
    "\n",
    "start_bold = \"\\u001b[1m\"\n",
    "end_bold = \"\\033[0m\"\n",
    "os.system(\"color\")\n",
    "color = {\n",
    "    \"ENDC\": end_bold,\n",
    "    \"Bold\": start_bold,\n",
    "}\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"save_log\", exist_ok=True)\n",
    "\n",
    "if dataset_name == 'CINIC10' or dataset_name == 'FEMNIST' or dataset_name == 'FEMNISTwriter':\n",
    "    neural_network_classname = f\"Neural_Network_CIFAR10_{neural_network_type}\"\n",
    "else:\n",
    "    neural_network_classname = f\"Neural_Network_{dataset_name}_{neural_network_type}\"\n",
    "\n",
    "\n",
    "if dataset_name == 'FEMNISTwriter':\n",
    "    num_clients = 3597 # or 'num_clients = len(writers)' but put fstring after 'writers' is fill\n",
    "\n",
    "save_file_name_pre = f\"\"\"FA\n",
    "_{dataset_name}_{neural_network_type}\n",
    "_{num_clients}c_{batch_size}b_{client_select_percentage}cp_{data_random_split}rs\n",
    "_{learning_rate}lr_{client_epochs}ce_step\"\"\"\n",
    "save_file_name_pre = save_file_name_pre.replace(\"\\n\", \"\")\n",
    "print(f\"save_log_file_name: '{save_file_name_pre}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_femnist_dataset_ready_to_read():\n",
    "    ### ToDo: Download and extract dataset to datasets dir\n",
    "    cur_dir = os.getcwd()\n",
    "    datasets_dir = os.path.join(cur_dir, \"datasets\")\n",
    "\n",
    "    if os.path.isdir(os.path.join(datasets_dir, \"by_class\")):\n",
    "        dataset_dir = os.path.join(datasets_dir, \"FEMNIST\")\n",
    "        os.rename(os.path.join(datasets_dir, \"by_class\"), dataset_dir)\n",
    "\n",
    "        for i, class_dir in enumerate(os.listdir(dataset_dir)):\n",
    "            class_imgs = os.path.join(dataset_dir, class_dir, \"train_\"+class_dir)\n",
    "            shutil.move(class_imgs, dataset_dir)\n",
    "            shutil.rmtree(os.path.join(dataset_dir, class_dir))\n",
    "            print(f\"Ready to be read and preprocess, class {i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "if dataset_name == 'FEMNIST':\n",
    "    download_femnist_dataset_ready_to_read()\n",
    "    \n",
    "    full_data = datasets.ImageFolder(\n",
    "        'datasets/FEMNIST',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=(32, 32)),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(62).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "    lst_range = np.arange(0, len(full_data))\n",
    "    lst_random = np.random.permutation(lst_range)\n",
    "    test_indices = lst_random[: int(len(lst_random)*0.1)]\n",
    "    train_indices = list(filter(lambda i: i not in test_indices, lst_range))\n",
    "\n",
    "    train_data = torch.utils.data.Subset(full_data, train_indices)\n",
    "    test_data = torch.utils.data.Subset(full_data, test_indices)\n",
    "\n",
    "elif dataset_name == 'FEMNISTwriter':\n",
    "    dataset_dir = \"datasets\\FEMNIST_by_write\\write_all_labels.hdf5\"\n",
    "    binary_data_file = h5py.File(dataset_dir, \"r\")\n",
    "\n",
    "    writers = sorted(binary_data_file.keys())\n",
    "    dic_train_indices = dict()\n",
    "    dic_test_indices = dict()\n",
    "    len_train_data = 0\n",
    "\n",
    "    for writer in writers:\n",
    "        labels = binary_data_file[writer]['labels']\n",
    "\n",
    "        lst_range = np.arange(0, len(labels))\n",
    "        lst_random = np.random.permutation(lst_range)\n",
    "        test_indices = lst_random[: int(len(lst_random)*0.1)]\n",
    "        train_indices = list(filter(lambda i: i not in test_indices, lst_range))\n",
    "        len_train_data += len(train_indices)\n",
    "\n",
    "        dic_train_indices[writer] = train_indices\n",
    "        dic_test_indices[writer] = test_indices\n",
    "\n",
    "elif dataset_name == 'CINIC10':\n",
    "    dataset_dir = 'datasets/CINIC-10'\n",
    "    cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "    cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "\n",
    "    train_data = datasets.ImageFolder(\n",
    "        dataset_dir + '/train_valid',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        dataset_dir + '/test',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "else: # 'MNIST' or 'CIFAR10'\n",
    "    running_dataset = getattr(datasets, dataset_name)\n",
    "\n",
    "    train_data = running_dataset(\n",
    "        root=\"datasets\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),    \n",
    "    )\n",
    "\n",
    "    test_data = running_dataset(\n",
    "        root=\"datasets\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "if dataset_name != 'FEMNISTwriter':\n",
    "    print(len(train_data))\n",
    "    assert len(train_data) < num_clients, (\"we expect each client have some data\")\n",
    "    print(train_data[0][0].shape)\n",
    "    print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name != 'FEMNISTwriter':\n",
    "    # Remove some data for running faster in test\n",
    "    print(\"remain data parameter:\", remain)\n",
    "    print(\"full train_data size:\", len(train_data))\n",
    "    train_data = torch.utils.data.Subset(train_data, range(0, int(len(train_data)*remain)))\n",
    "    print(\"cutted train_data size:\", len(train_data))\n",
    "\n",
    "    print(\"full test_data size:\", len(test_data))\n",
    "    test_data = torch.utils.data.Subset(test_data, range(0, int(len(test_data)*remain)))\n",
    "    print(\"cutted test_data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_clients in FEMNISTwriter: 3597\n",
      "734464\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Split data to clients\n",
    "if dataset_name != 'FEMNISTwriter':\n",
    "    if clients_data_distribution == \"equal\":\n",
    "        client_data_size = np.array([len(train_data)//num_clients]*num_clients)\n",
    "        data_remain = len(train_data) % num_clients\n",
    "        for i in range(data_remain):\n",
    "            client_data_size[-1-i] += 1\n",
    "    elif clients_data_distribution == \"random\":\n",
    "        # random numbers with sum=1\n",
    "        random_numbers = np.random.dirichlet(np.ones(num_clients))\n",
    "        # set a min_thresh: ref='https://stackoverflow.com/a/62911965/4464934'\n",
    "        min_thresh = 1 / len(train_data)\n",
    "        rand_prop = 1 - num_clients * min_thresh\n",
    "        random_numbers = (random_numbers * rand_prop) + min_thresh\n",
    "        client_data_size = np.round(random_numbers*len(train_data))\n",
    "\n",
    "\n",
    "    if data_random_split:\n",
    "        client_datasets = torch.utils.data.random_split(train_data, client_data_size)\n",
    "    else:\n",
    "        client_datasets = list()\n",
    "        i = 0\n",
    "        for j in client_data_size:\n",
    "            client_datasets.append(torch.utils.data.Subset(train_data, range(i, i+j)))\n",
    "            i += j\n",
    "else:\n",
    "    client_datasets = get_client_datasets(writers, binary_data_file, dic_train_indices)\n",
    "    test_data = get_test_dataset(writers, binary_data_file, dic_test_indices)\n",
    "\n",
    "    print(f\"num_clients in FEMNISTwriter: {len(client_datasets)}\")\n",
    "    print(len_train_data)\n",
    "    print(client_datasets[0][0][0].shape)\n",
    "    print(client_datasets[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for each client\n",
    "client_dataloaders = np.zeros(num_clients, dtype=object)\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    client_dataloaders[i] = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                       shuffle=True, generator=dataloader_generator,)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,\n",
    "                             shuffle=True, generator=dataloader_generator,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_conv_max_layers(in_w, in_h, kernels, strides, paddings=None, dilations=None):\n",
    "    # In MaxPool2d, strides must same with kernels\n",
    "    if paddings == None:\n",
    "        paddings = np.zeros(len(kernels))\n",
    "    \n",
    "    if dilations == None:\n",
    "        dilations = np.ones(len(kernels))\n",
    "    \n",
    "    out_w = in_w\n",
    "    out_h = in_h\n",
    "    for ker, pad, dil, stri in zip(kernels, paddings, dilations, strides):\n",
    "        out_w = np.floor((out_w + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "        out_h = np.floor((out_h + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "\n",
    "    return int(out_w), int(out_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP models\n",
    "input_flat_size = torch.flatten(test_data[0][0]).shape[0]\n",
    "nClasses = test_data[0][1].shape[0]\n",
    "\n",
    "class Neural_Network_MNIST_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 100)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(100, 99)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(99, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "class Neural_Network_CIFAR10_MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 256)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(256, 128)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(128, 64)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(64, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Convolutional models\n",
    "input_channels, input_width, input_height = test_data[0][0].shape\n",
    "conv_kernel = 3\n",
    "max_kernel = 2\n",
    "\n",
    "kernels = [conv_kernel, max_kernel, conv_kernel]\n",
    "strides = [1, max_kernel, 1]\n",
    "out_w1, out_h1 = calc_out_conv_max_layers(input_width, input_height, kernels, strides)\n",
    "\n",
    "class Neural_Network_MNIST_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel, stride=1, padding=0)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel)),\n",
    "            ('conv2', nn.Conv2d(32, 64, kernel_size=conv_kernel)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w1*out_h1, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "kernels = [conv_kernel, max_kernel, conv_kernel, max_kernel]\n",
    "strides = [1, max_kernel, 1, max_kernel]\n",
    "out_w2, out_h2 = calc_out_conv_max_layers(input_width, input_height, kernels, strides)\n",
    "\n",
    "class Neural_Network_CIFAR10_Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel, stride=1, padding=0)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel)),\n",
    "            ('conv2', nn.Conv2d(32, 64, kernel_size=conv_kernel)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w2*out_h2, 100)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('fc2', nn.Linear(100, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clients_and_assign_weights(global_weights):\n",
    "    global client_selects\n",
    "    global client_weights\n",
    "\n",
    "    lst = np.arange(0, num_clients)\n",
    "    np.random.shuffle(lst)\n",
    "    client_selects = lst[: int(len(lst)*client_select_percentage)]\n",
    "\n",
    "    client_weights = {i: copy.deepcopy(global_weights)  for i in client_selects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Network_CIFAR10_Conv(\n",
      "  (features_stack): Sequential(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=1600, out_features=100, bias=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (fc2): Linear(in_features=100, out_features=62, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instantiate of a class with string value!\n",
    "global_model = globals()[neural_network_classname]().to(device)\n",
    "global_weights = global_model.get_weights()\n",
    "select_clients_and_assign_weights(global_weights)\n",
    "print(global_model)\n",
    "\n",
    "global_history = {\"times\": {\"train\":list(), \"step\":list()},\n",
    "                  \"accuracy\": list(),\n",
    "                  \"loss\": list()}\n",
    "\n",
    "# Load saved state & log\n",
    "last_saved = sorted(glob.glob(f\"save_log/{save_file_name_pre}_*.npz\"), key=os.path.getmtime)\n",
    "if last_saved:\n",
    "    last_saved = last_saved[-1]\n",
    "    passed_steps = int(last_saved.split(\"_\")[-1].split(\".\")[0]) + 1\n",
    "\n",
    "    npzFile = np.load(last_saved, allow_pickle=True)\n",
    "    client_selects = npzFile[\"client_selects\"]\n",
    "    client_weights = npzFile[\"client_weights\"].item()\n",
    "    global_history = npzFile[\"global_history\"].item()\n",
    "    dataloader_generator_state = torch.tensor(npzFile[\"dataloader_generator_state\"])\n",
    "    random_state = tuple(npzFile[\"random_state_ndarray\"])\n",
    "    np_random_state = tuple(npzFile[\"np_random_state_ndarray\"])\n",
    "    # torch_rng_states = npzFile[\"torch_rng_states_ndarray\"]\n",
    "    npzFile.close()\n",
    "\n",
    "    dataloader_generator.set_state(dataloader_generator_state)\n",
    "    random.setstate(random_state)\n",
    "    np.random.set_state(np_random_state)\n",
    "    # torch.set_rng_state(torch_rng_states[0])\n",
    "    # torch.cuda.set_rng_state(torch_rng_states[1])\n",
    "    # torch.cuda.set_rng_state_all(torch_rng_states[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weights, scalar):\n",
    "    \"\"\" Scale the model weights \"\"\"\n",
    "\n",
    "    scaled_weights = list()\n",
    "    for i in range(len(weights)):\n",
    "        scaled_weights.append(weights[i] * scalar)\n",
    "\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(client_scaled_weights):\n",
    "    \"\"\" Return the sum of the listed scaled weights.\n",
    "        axis_O is equivalent to the average weight of the weights \"\"\"\n",
    "\n",
    "    avg_weights = list()\n",
    "    # get the average gradient accross all client gradients\n",
    "    for gradient_list_tuple in zip(*client_scaled_weights):\n",
    "        gradient_list_tuple = [tensors.tolist()  for tensors in gradient_list_tuple]\n",
    "        layer_mean = torch.sum(torch.tensor(gradient_list_tuple), axis=0).to(device)\n",
    "        avg_weights.append(layer_mean)\n",
    "\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "### Explaining the function with example ###\n",
    "# t = (torch.tensor([[[2, 3],[3, 4]], [[3, 4],[4, 5]], [[4, 5],[5, 6]]]),\n",
    "#      torch.tensor([[[5, 6],[6, 7]], [[6, 7],[7, 8]], [[7, 8],[8, 9]]]))\n",
    "# t = [i.tolist() for i in t]\n",
    "# for y in zip(*t):\n",
    "#     print(y)\n",
    "#     print(torch.sum(torch.tensor(y), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg():\n",
    "    # calculate total training data across clients\n",
    "    global_count = 0\n",
    "    for client in client_selects:\n",
    "        global_count += len(client_dataloaders[client].dataset)\n",
    "\n",
    "    # initial list to collect clients weight after scalling\n",
    "    client_scaled_weights = list()\n",
    "    for client in client_selects:\n",
    "        local_count = len(client_dataloaders[client].dataset)\n",
    "        scaling_factor = local_count / global_count\n",
    "        scaled_weights = scale_model_weights(client_weights[client], scaling_factor)\n",
    "        client_scaled_weights.append(scaled_weights)\n",
    "\n",
    "    # to get the average over all the clients model, we simply take the sum of the scaled weights\n",
    "    avg_weights = sum_scaled_weights(client_scaled_weights)\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct_items = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_items += (pred.argmax(1) == y.argmax(1)).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / num_batches\n",
    "    accuracy = correct_items / data_size\n",
    "    # print(f\"Test Error: \\nAccuracy: {(accuracy*100):>0.1f}%, Loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print_step = np.ceil(len(dataloader)/10)\n",
    "        if batch % print_step == 0:\n",
    "            loss_per_batch = running_loss / print_step\n",
    "            current_item = (batch+1)*len(x)\n",
    "            # print(f\"loss: {loss_per_batch:>7f}  [{current_item:>5d}/{data_size:>5d}]\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clinet(dataloader, model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(client_epochs):\n",
    "        train_neural_network(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_and_log(step):\n",
    "    # save torch_rng, if you need that\n",
    "    # torch_rng_states = [torch.get_rng_state(), torch.cuda.get_rng_state(), torch.cuda.get_rng_state_all()]\n",
    "    # torch_rng_states_ndarray = np.array(torch_rng_states, dtype=object)\n",
    "\n",
    "    dataloader_generator_state = dataloader_generator.get_state()\n",
    "    random_state = random.getstate()\n",
    "    np_random_state = np.random.get_state()\n",
    "    random_state_ndarray = np.array(random_state, dtype=object)\n",
    "    np_random_state_ndarray = np.array(np_random_state, dtype=object)\n",
    "\n",
    "    np.savez_compressed(f\"save_log/{save_file_name_pre}_{step}.npz\",\n",
    "                        client_selects=client_selects,\n",
    "                        client_weights=client_weights,\n",
    "                        global_history=global_history,\n",
    "                        dataloader_generator_state=dataloader_generator_state,\n",
    "                        random_state_ndarray=random_state_ndarray,\n",
    "                        np_random_state_ndarray=np_random_state_ndarray,\n",
    "                        # torch_rng_states_ndarray=torch_rng_states_ndarray,\n",
    "                        )\n",
    "\n",
    "    if step != 0:\n",
    "        os.remove(f\"save_log/{save_file_name_pre}_{step-1}.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_time_format(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    if h:\n",
    "        return f\"{h:.0f}h-{m:.0f}m-{s:.0f}s\"\n",
    "    elif m:\n",
    "        return f\"{m:.0f}m-{s:.0f}s\"\n",
    "    else:\n",
    "        return f\"{s:.2f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(training_time, step_time, step, metric_index=-1):\n",
    "    training_time = change_time_format(training_time)\n",
    "    step_time = change_time_format(step_time)\n",
    "    print(f\"round: {step} | training_time: {training_time} | step_time: {step_time}\")\n",
    "    print(f\"round: {step} / global_acc: {start_bold}{global_history['accuracy'][metric_index]:.4%}{end_bold} / global_loss: {start_bold}{global_history['loss'][metric_index]:.4f}{end_bold}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prev_log(passed_steps):\n",
    "    if passed_steps:\n",
    "        for step in range(passed_steps):\n",
    "            training_time = global_history[\"times\"][\"train\"][step]\n",
    "            step_time = global_history[\"times\"][\"step\"][step]\n",
    "            print_log(training_time, step_time, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FedAvg Main Loop\n",
    "print_prev_log(passed_steps)\n",
    "for step in range(passed_steps, total_steps):\n",
    "    training_time_start = time.time()\n",
    "    for client in client_selects:\n",
    "        local_model = globals()[neural_network_classname]().to(device)\n",
    "        local_model.set_weights(client_weights[client])\n",
    "        train_clinet(client_dataloaders[client], local_model)\n",
    "        client_weights[client] = local_model.get_weights()\n",
    "\n",
    "        del local_model\n",
    "    \n",
    "    training_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"train\"].append(training_time)\n",
    "\n",
    "\n",
    "    avg_weights = fed_avg()\n",
    "    global_model.set_weights(avg_weights) # update global model\n",
    "    select_clients_and_assign_weights(avg_weights)\n",
    "    global_acc, global_loss = test_neural_network(test_dataloader, global_model, loss_fn)\n",
    "    global_history[\"accuracy\"].append(global_acc)\n",
    "    global_history[\"loss\"].append(global_loss)\n",
    "    \n",
    "    \n",
    "    step_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"step\"].append(step_time)\n",
    "    print_log(training_time, step_time, step)\n",
    "    save_state_and_log(step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"loss\"], label=\"test loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"accuracy\"], label=\"test accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"loss\"], label=\"test loss\")\n",
    "# plt.plot(global_history[\"accuracy\"], label=\"test accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss / Accuracy\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
