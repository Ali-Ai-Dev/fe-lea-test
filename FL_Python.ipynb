{"cells":[{"cell_type":"code","execution_count":193,"metadata":{"executionInfo":{"elapsed":1147,"status":"ok","timestamp":1705491421935,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"hMM6D1tDTi4y"},"outputs":[],"source":["# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=975848d46b9b21c8c1e3f4c0bbbcf7c71331c143233d64e445bc7899244215fe\"\n","# name = url.split(\"/\")[-1]\n","# fileName = name.split(\"?\")[0]\n","\n","# !wget $url\n","# !mv $name $fileName"]},{"cell_type":"code","execution_count":194,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1705491421936,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"sGtX5o7VTi41"},"outputs":[],"source":["# %%capture\n","# !unzip mnist.zip;"]},{"cell_type":"code","execution_count":195,"metadata":{"executionInfo":{"elapsed":47,"status":"ok","timestamp":1705491421937,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"RzTrKyXWTi41"},"outputs":[],"source":["import numpy as np\n","import random\n","import cv2\n","import os\n","from imutils import paths\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Activation\n","from keras.layers import Dense\n","from keras import optimizers\n","from keras import backend as K"]},{"cell_type":"code","execution_count":196,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1705491421937,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"EJTwAY79MZbw"},"outputs":[],"source":["mySeed = 42\n","np.random.seed(mySeed)\n","random.seed(mySeed)\n","tf.random.set_seed(mySeed)\n","# torch.manual_seed(mySeed)"]},{"cell_type":"code","execution_count":197,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1705491421938,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"XpvfFXbOTi42"},"outputs":[],"source":["def load_mnist_bypath(paths, verbose=-1):\n","    \"\"\" Expect to read images where each class is in a separate directory,\n","        For example: images of type 0 are in folder 0\n","    \"\"\"\n","\n","    data = list()\n","    labels = list()\n","    # loop over the input images\n","    for (i, imgpath) in enumerate(paths):\n","        # load the image and extract the class labels\n","        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n","        image = np.array(im_gray).flatten()\n","        label = imgpath.split(os.path.sep)[-2]\n","        # here the img is scaled to [0, 1] to less the impact of each pixel's brightness\n","        data.append(image/255)\n","        labels.append(label)\n","        # show an update every `verbose` images\n","        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n","            print(f\"[INFO] processed {i+1}/{len(paths)}\")\n","            break\n","\n","    # return a tuple of the data and labels\n","    return data, labels"]},{"cell_type":"code","execution_count":198,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":735,"status":"ok","timestamp":1705491422629,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"K8GgAuQUTi42","outputId":"ac9de5a9-d2b8-4db6-ac3e-a9b66b1d79b3"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] processed 10000/42000\n"]}],"source":["# Declare mnist dataset path\n","img_path = \"../mnist/trainingSet/trainingSet\"\n","# img_path = \"/content/mnist/trainingSet/trainingSet\"\n","\n","# Generate a list of trucks using the list_images function from the paths library\n","image_paths = list(paths.list_images(img_path))\n","\n","# load images into arrays\n","image_list, label_list = load_mnist_bypath(image_paths, verbose=10000)\n","\n","# Perform the one-hot encoded so we can use the sparse-categorical-entropy loss function\n","lb = LabelBinarizer()\n","label_list = lb.fit_transform(label_list)\n","\n","#split data into training and test set\n","X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":199,"metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1705491422666,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"4LqxZDHMTi43"},"outputs":[],"source":["def create_clients(image_list, label_list, num_clients=10, initial=\"clients\"):\n","    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n","                will be the data fragment - tuple of images and labels.\n","        args:\n","            image_list: a numpy array object with the images\n","            label_list: list of binarized labels (one-hot encoded)\n","            num_client: number of customers (clients)\n","            initials: the prefix of the clients, e.g., clients_1\n","     \"\"\"\n","\n","    # create list of customer names\n","    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n","\n","    # shuffle the data\n","    data = list(zip(image_list, label_list))\n","    random.shuffle(data)\n","\n","    # shard the data and split it for each customer\n","    size = len(data) // num_clients\n","    shards = [data[i: i+size] for i in range(0, size * num_clients, size)]\n","\n","    # Check if the fragment number is equal to the number of clients\n","    assert(len(shards) == len(client_names))\n","\n","    return {client_names[i]: shards[i] for i in range(len(client_names))}\n","\n","\n","num_clients = 2\n","# Create the customers\n","clients = create_clients(X_train, y_train, num_clients=num_clients, initial=\"client\")"]},{"cell_type":"code","execution_count":200,"metadata":{"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1705491423999,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"_xXnY6tFTi43"},"outputs":[],"source":["def batch_data(data_shard, b=32):\n","    \"\"\" Receives a piece of data from a client and creates a tensorflow data object in it\n","        args:\n","            data_shard: data and labels that make up a customer's data shard\n","            b: batch size\n","        return:\n","            data tensorflow object\n","    \"\"\"\n","    #seperate shard into data and labels lists\n","    data, label = zip(*data_shard)\n","    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n","    return dataset.shuffle(len(label), reshuffle_each_iteration=False).batch(b)\n","\n","\n","# Process and collate the training data for each client\n","clients_batched = dict()\n","for (client_name, data) in clients.items():\n","    clients_batched[client_name] = batch_data(data)\n","\n","# process and group test set\n","test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"]},{"cell_type":"code","execution_count":201,"metadata":{"executionInfo":{"elapsed":178,"status":"ok","timestamp":1705491424069,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"xe1ZFi2gTi44"},"outputs":[],"source":["class MLP:\n","    @staticmethod\n","    def build(shape, classes):\n","        model = Sequential()\n","        model.add(Dense(100, input_shape=(shape,)))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(100))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(classes))\n","        model.add(Activation(\"softmax\"))\n","\n","        return model\n","\n","lr = 0.01\n","comms_round = 2\n","# loss = \"categorical_crossentropy\"\n","loss_cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","metrics = [\"accuracy\"]\n","# optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)\n","# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=lr/comms_round, momentum=0.9)\n","\n","# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)"]},{"cell_type":"code","execution_count":202,"metadata":{"executionInfo":{"elapsed":177,"status":"ok","timestamp":1705491424070,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"LiDTqgXgTi44"},"outputs":[],"source":["def weight_scalling_factor(clients_trn_data, client_name, participants):\n","    \"\"\" Calculates the size ratio of a client's local training data\n","        with all general training data maintained by all customers\n","    \"\"\"\n","    # client_names = list(clients_trn_data.keys())\n","    # calculate batch size\n","    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n","    # first calculate total training data across clients\n","    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()\n","                        for client_name in participants]) * bs\n","    # get the total number of data points held by a client\n","    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * bs\n","\n","    return local_count / global_count"]},{"cell_type":"code","execution_count":203,"metadata":{"executionInfo":{"elapsed":177,"status":"ok","timestamp":1705491424072,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"7wFLVmqtTi44"},"outputs":[],"source":["def scale_model_weights(weight, scalar):\n","    \"\"\" Scale the model weights \"\"\"\n","    weight_final = []\n","    steps = len(weight)\n","    for i in range(steps):\n","        weight_final.append(scalar * weight[i])\n","\n","    return weight_final"]},{"cell_type":"code","execution_count":204,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1705491424074,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"uTH6M7XCTi45"},"outputs":[],"source":["def sum_scaled_weights(scaled_weight_list):\n","    \"\"\" Return the sum of the listed scaled weights. O is equivalent to the average weight of the weights \"\"\"\n","    avg_grad = list()\n","    # get the average grad accross all client gradients\n","    for grad_list_tuple in zip(*scaled_weight_list):\n","        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n","        avg_grad.append(layer_mean)\n","\n","    return avg_grad"]},{"cell_type":"code","execution_count":205,"metadata":{"executionInfo":{"elapsed":176,"status":"ok","timestamp":1705491424077,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"JJNKf44FTi45"},"outputs":[],"source":["def test_model(X_test, Y_test, model, comm_round):\n","    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","    # logits = model.predict(X_test, batch_size=100)\n","    logits = model.predict(X_test)\n","    loss = cce(Y_test, logits)\n","    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n","    # print(f\"Agregation Round: {comm_round} | global_acc: {acc:.3%} | global_loss: {loss}\"\")\n","    print(f\"round: {comm_round} | global_acc: {acc:.4%} | global_loss: {loss:.4} \\n\")\n","\n","    return acc, loss"]},{"cell_type":"code","execution_count":206,"metadata":{"executionInfo":{"elapsed":175,"status":"ok","timestamp":1705491424078,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"k8wrKPisTi45"},"outputs":[],"source":["def check_local_loss(client, model):\n","    # Check local loss\n","    cce_l = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","    client_x = np.array([i[0] for i in clients[client]])\n","    client_y = np.array([i[1] for i in clients[client]])\n","    logits_l = model.predict(client_x, verbose=0)\n","    loss_l = cce_l(client_y, logits_l)\n","    acc_l = accuracy_score(tf.argmax(logits_l, axis=1), tf.argmax(client_y, axis=1))\n","    print(f\"local accuracy: {acc_l:.4} | local loss: {loss_l:.4}\")\n","\n","    return acc_l, loss_l"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4838,"status":"ok","timestamp":1705491428744,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"},"user_tz":-210},"id":"SZBZ4LjTTi45","outputId":"8f40ab24-2a63-4ab6-ddb8-eb1192075593"},"outputs":[{"name":"stdout","output_type":"stream","text":["local accuracy: 0.9885 | local loss: 0.03527\n","local accuracy: 0.9965 | local loss: 0.0114\n","63/63 [==============================] - 0s 4ms/step\n","round: 0 | global_acc: 98.9000% | global_loss: 0.05843 \n","\n","local accuracy: 0.9945 | local loss: 0.01702\n","local accuracy: 0.994 | local loss: 0.02153\n","63/63 [==============================] - 5s 85ms/step\n","round: 1 | global_acc: 99.5000% | global_loss: 0.01714 \n","\n"]}],"source":["client_select_rate = 0.6\n","client_select_rate = 1\n","client_epochs = 1\n","### Start global template ###\n","\n","smlp_global = MLP()\n","global_model = smlp_global.build(784, np.array(y_train).shape[-1])\n","\n","# Global training loop collection\n","for comm_round in range(comms_round):\n","\n","    # get the global model's weights - will serve as the initial weights for all local models\n","    global_weights = global_model.get_weights()\n","\n","    # initial list to collect local model weights after scalling\n","    scaled_local_weight_list = list()\n","\n","    # randomize client data - using keys\n","    client_names = list(clients_batched.keys())\n","    random.shuffle(client_names)\n","    client_select = client_names[0: int(num_clients*client_select_rate)]\n","\n","    # loop through each client and create a new local model\n","    for client in client_select:\n","        smlp_local = MLP()\n","        local_model = smlp_local.build(784, np.array(y_train).shape[-1])\n","        local_model.compile(loss=loss_cce, optimizer=optimizer, metrics=metrics)\n","\n","        # set the weight of the local model to the weight of the global model\n","        local_model.set_weights(global_weights)\n","\n","        # fit local model with client's data\n","        local_model.fit(clients_batched[client], epochs=client_epochs, verbose=0)\n","\n","        # scale the model weights and add to list\n","        scaling_factor = weight_scalling_factor(clients_batched, client, client_select)\n","        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n","        scaled_local_weight_list.append(scaled_weights)\n","\n","        # Check local accuracy\n","        acc_l, loss_l = check_local_loss(client, local_model)\n","\n","        # clear session to free memory after each communication round\n","        K.clear_session()\n","\n","    # to get the average over all the local model, we simply take the sum of the scaled weights\n","    average_weights = sum_scaled_weights(scaled_local_weight_list)\n","\n","    # # update global model\n","    global_model.set_weights(average_weights)\n","    \n","\n","    # test global model and print out metrics after each communications round\n","    for (X_test, Y_test) in test_batched:\n","        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
