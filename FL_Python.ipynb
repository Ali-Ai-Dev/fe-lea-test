{"cells":[{"cell_type":"code","execution_count":22,"metadata":{"id":"hMM6D1tDTi4y","executionInfo":{"status":"ok","timestamp":1705491421935,"user_tz":-210,"elapsed":1147,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=3f7be9d95817f1971e422164245710432fd335df5f760342b8641c0db02b6265\"\n","# name = url.split(\"/\")[-1]\n","# fileName = name.split(\"?\")[0]\n","\n","# !wget $url\n","# !mv $name $fileName"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"sGtX5o7VTi41","executionInfo":{"status":"ok","timestamp":1705491421936,"user_tz":-210,"elapsed":47,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["# %%capture\n","# !unzip mnist.zip;"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"RzTrKyXWTi41","executionInfo":{"status":"ok","timestamp":1705491421937,"user_tz":-210,"elapsed":47,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["import numpy as np\n","import random\n","import cv2\n","import os\n","from imutils import paths\n","from sklearn.preprocessing import LabelBinarizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Activation\n","from keras.layers import Dense\n","from keras import optimizers\n","from keras import backend as K"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"EJTwAY79MZbw","executionInfo":{"status":"ok","timestamp":1705491421937,"user_tz":-210,"elapsed":45,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["mySeed = 42\n","np.random.seed(mySeed)\n","random.seed(mySeed)\n","tf.random.set_seed(mySeed)\n","# torch.manual_seed(mySeed)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"XpvfFXbOTi42","executionInfo":{"status":"ok","timestamp":1705491421938,"user_tz":-210,"elapsed":45,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def load_mnist_bypath(paths, verbose=-1):\n","    \"\"\" Expect to read images where each class is in a separate directory,\n","        For example: images of type 0 are in folder 0\n","    \"\"\"\n","\n","    data = list()\n","    labels = list()\n","    # loop over the input images\n","    for (i, imgpath) in enumerate(paths):\n","        # load the image and extract the class labels\n","        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n","        image = np.array(im_gray).flatten()\n","        label = imgpath.split(os.path.sep)[-2]\n","        # here the img is scaled to [0, 1] to less the impact of each pixel's brightness\n","        data.append(image/255)\n","        labels.append(label)\n","        # show an update every `verbose` images\n","        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n","            print(f\"[INFO] processed {i+1}/{len(paths)}\")\n","            break\n","\n","    # return a tuple of the data and labels\n","    return data, labels"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K8GgAuQUTi42","outputId":"ac9de5a9-d2b8-4db6-ac3e-a9b66b1d79b3","executionInfo":{"status":"ok","timestamp":1705491422629,"user_tz":-210,"elapsed":735,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] processed 10000/42000\n"]}],"source":["# Declare mnist dataset path\n","img_path = \"mnist/trainingSet/trainingSet\"\n","img_path = \"/content/mnist/trainingSet/trainingSet\"\n","\n","# Generate a list of trucks using the list_images function from the paths library\n","image_paths = list(paths.list_images(img_path))\n","\n","# load images into arrays\n","image_list, label_list = load_mnist_bypath(image_paths, verbose=10000)\n","\n","# Perform the one-hot encoded so we can use the sparse-categorical-entropy loss function\n","lb = LabelBinarizer()\n","label_list = lb.fit_transform(label_list)\n","\n","#split data into training and test set\n","X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"4LqxZDHMTi43","executionInfo":{"status":"ok","timestamp":1705491422666,"user_tz":-210,"elapsed":221,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def create_clients(image_list, label_list, num_clients=10, initial=\"clients\"):\n","    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n","                will be the data fragment - tuple of images and labels.\n","        args:\n","            image_list: a numpy array object with the images\n","            label_list: list of binarized labels (one-hot encoded)\n","            num_client: number of customers (clients)\n","            initials: the prefix of the clients, e.g., clients_1\n","     \"\"\"\n","\n","    # create list of customer names\n","    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n","\n","    # shuffle the data\n","    data = list(zip(image_list, label_list))\n","    random.shuffle(data)\n","\n","    # shard the data and split it for each customer\n","    size = len(data) // num_clients\n","    shards = [data[i: i+size] for i in range(0, size * num_clients, size)]\n","\n","    # Check if the fragment number is equal to the number of clients\n","    assert(len(shards) == len(client_names))\n","\n","    return {client_names[i]: shards[i] for i in range(len(client_names))}\n","\n","\n","num_clients = 2\n","# Create the customers\n","clients = create_clients(X_train, y_train, num_clients=num_clients, initial=\"client\")"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"_xXnY6tFTi43","executionInfo":{"status":"ok","timestamp":1705491423999,"user_tz":-210,"elapsed":1551,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def batch_data(data_shard, b=32):\n","    \"\"\" Receives a piece of data from a client and creates a tensorflow data object in it\n","        args:\n","            data_shard: data and labels that make up a customer's data shard\n","            b: batch size\n","        return:\n","            data tensorflow object\n","    \"\"\"\n","    #seperate shard into data and labels lists\n","    data, label = zip(*data_shard)\n","    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n","    return dataset.shuffle(len(label)).batch(b)\n","\n","\n","# Process and collate the training data for each client\n","clients_batched = dict()\n","for (client_name, data) in clients.items():\n","    clients_batched[client_name] = batch_data(data)\n","\n","# process and group test set\n","test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"xe1ZFi2gTi44","executionInfo":{"status":"ok","timestamp":1705491424069,"user_tz":-210,"elapsed":178,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["class MLP:\n","    @staticmethod\n","    def build(shape, classes):\n","        model = Sequential()\n","        model.add(Dense(100, input_shape=(shape,)))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(100))\n","        model.add(Activation(\"relu\"))\n","        model.add(Dense(classes))\n","        model.add(Activation(\"softmax\"))\n","\n","        return model\n","\n","lr = 0.01\n","comms_round = 1\n","loss = \"categorical_crossentropy\"\n","metrics = [\"accuracy\"]\n","# optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)\n","# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=lr/comms_round, momentum=0.9)\n","\n","# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n","optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"LiDTqgXgTi44","executionInfo":{"status":"ok","timestamp":1705491424070,"user_tz":-210,"elapsed":177,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def weight_scalling_factor(clients_trn_data, client_name, participants):\n","    \"\"\" Calculates the size ratio of a client's local training data\n","        with all general training data maintained by all customers\n","    \"\"\"\n","    # client_names = list(clients_trn_data.keys())\n","    # calculate batch size\n","    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n","    # first calculate total training data across clients\n","    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()\n","                        for client_name in participants]) * bs\n","    # get the total number of data points held by a client\n","    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * bs\n","\n","    return local_count / global_count"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"7wFLVmqtTi44","executionInfo":{"status":"ok","timestamp":1705491424072,"user_tz":-210,"elapsed":177,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def scale_model_weights(weight, scalar):\n","    \"\"\" Scale the model weights \"\"\"\n","    weight_final = []\n","    steps = len(weight)\n","    for i in range(steps):\n","        weight_final.append(scalar * weight[i])\n","\n","    return weight_final"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"uTH6M7XCTi45","executionInfo":{"status":"ok","timestamp":1705491424074,"user_tz":-210,"elapsed":176,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def sum_scaled_weights(scaled_weight_list):\n","    \"\"\" Return the sum of the listed scaled weights. O is equivalent to the average weight of the weights \"\"\"\n","    avg_grad = list()\n","    # get the average grad accross all client gradients\n","    for grad_list_tuple in zip(*scaled_weight_list):\n","        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n","        avg_grad.append(layer_mean)\n","\n","    return avg_grad"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"JJNKf44FTi45","executionInfo":{"status":"ok","timestamp":1705491424077,"user_tz":-210,"elapsed":176,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def test_model(X_test, Y_test, model, comm_round):\n","    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n","    # logits = model.predict(X_test, batch_size=100)\n","    logits = model.predict(X_test)\n","    loss = cce(Y_test, logits)\n","    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n","    # print(f\"Agregation Round: {comm_round} | global_acc: {acc:.3%} | global_loss: {loss}\"\")\n","    print(f\"round: {comm_round} | acc: {acc:.3%} | loss: {loss}\")\n","\n","    return acc, loss"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"k8wrKPisTi45","executionInfo":{"status":"ok","timestamp":1705491424078,"user_tz":-210,"elapsed":175,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":["def check_local_loss(client, model):\n","    # Check local loss\n","    cce_l = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n","    client_x = np.array([i[0] for i in clients[client]])\n","    client_y = np.array([i[1] for i in clients[client]])\n","    logits_l = model.predict(client_x, verbose=0)\n","    loss_l = cce_l(client_y, logits_l)\n","    acc_l = accuracy_score(tf.argmax(logits_l, axis=1), tf.argmax(client_y, axis=1))\n","    print(f\"Local accuracy: {acc_l}. Local loss: {loss_l}\")\n","\n","    return acc_l, loss_l"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZBZ4LjTTi45","outputId":"8f40ab24-2a63-4ab6-ddb8-eb1192075593","executionInfo":{"status":"ok","timestamp":1705491428744,"user_tz":-210,"elapsed":4838,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Local accuracy: 0.9885. Local loss: 0.03640047460794449\n","Local accuracy: 0.9825. Local loss: 0.053259432315826416\n"]}],"source":["client_select_rate = 0.6\n","client_select_rate = 1\n","client_epochs = 1\n","### Start global template ###\n","\n","smlp_global = MLP()\n","global_model = smlp_global.build(784, np.array(y_train).shape[-1])\n","\n","# Global training loop collection\n","for comm_round in range(comms_round):\n","\n","    # get the global model's weights - will serve as the initial weights for all local models\n","    global_weights = global_model.get_weights()\n","\n","    # initial list to collect local model weights after scalling\n","    scaled_local_weight_list = list()\n","\n","    # randomize client data - using keys\n","    client_names = list(clients_batched.keys())\n","    random.shuffle(client_names)\n","    client_select = client_names[0: int(num_clients*client_select_rate)]\n","\n","    # loop through each client and create a new local model\n","    for client in client_select:\n","        smlp_local = MLP()\n","        local_model = smlp_local.build(784, np.array(y_train).shape[-1])\n","        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n","\n","        # set the weight of the local model to the weight of the global model\n","        local_model.set_weights(global_weights)\n","\n","        # fit local model with client's data\n","        local_model.fit(clients_batched[client], epochs=client_epochs, verbose=0)\n","\n","        # scale the model weights and add to list\n","        scaling_factor = weight_scalling_factor(clients_batched, client, client_select)\n","        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n","        scaled_local_weight_list.append(scaled_weights)\n","\n","        # Check local accuracy\n","        acc_l, loss_l = check_local_loss(client, local_model)\n","\n","        # clear session to free memory after each communication round\n","        K.clear_session()\n","\n","    # to get the average over all the local model, we simply take the sum of the scaled weights\n","    average_weights = sum_scaled_weights(scaled_local_weight_list)\n","\n","    # # update global model\n","    global_model.set_weights(average_weights)\n","\n","    # # test global model and print out metrics after each communications round\n","    # for (X_test, Y_test) in test_batched:\n","    #     global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YW8IJLMxMZb-","executionInfo":{"status":"ok","timestamp":1705491455973,"user_tz":-210,"elapsed":484,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}},"outputId":"a9c69514-41fc-41e9-f74b-a5bfa79e5527"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(784, 100), dtype=float32, numpy=\n","array([[ 0.12132786,  0.1221748 ,  0.01439963, ...,  0.0271544 ,\n","         0.05098383, -0.11390366],\n","       [-0.04353974,  0.08795384, -0.11680636, ...,  0.00224975,\n","         0.0975301 ,  0.09754964],\n","       [-0.01309502, -0.01776013, -0.06079351, ..., -0.02687726,\n","         0.01124474, -0.09187619],\n","       ...,\n","       [ 0.08114747, -0.12404118, -0.06668322, ..., -0.05489971,\n","         0.06418204,  0.06458349],\n","       [-0.02142312,  0.01723315,  0.04612453, ...,  0.02928697,\n","         0.03162291,  0.03569558],\n","       [ 0.07391876, -0.10330684,  0.05180462, ...,  0.00726108,\n","         0.05258062, -0.01455349]], dtype=float32)>"]},"metadata":{},"execution_count":38}],"source":["average_weights[0]"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"6-2irHDGMZb_","executionInfo":{"status":"ok","timestamp":1705491428746,"user_tz":-210,"elapsed":47,"user":{"displayName":"Ali Bozorgzad","userId":"14334497886659363986"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}