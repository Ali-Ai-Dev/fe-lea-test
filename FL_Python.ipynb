{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hMM6D1tDTi4y"
      },
      "outputs": [],
      "source": [
        "# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=508c1064daf4166bb88aebfc9816ce51b4ba3b27a5b429f59febb868754c005e\"\n",
        "# fileName = url.split(\"/\")[-1]\n",
        "\n",
        "# !wget \"https://usaupload.com/72Eb/mnist.zip?download_token=508c1064daf4166bb88aebfc9816ce51b4ba3b27a5b429f59febb868754c005e\"\n",
        "# !mv \"mnist.zip?download_token=508c1064daf4166bb88aebfc9816ce51b4ba3b27a5b429f59febb868754c005e\" \"mnist.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "sGtX5o7VTi41"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !unzip mnist.zip;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RzTrKyXWTi41"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XpvfFXbOTi42"
      },
      "outputs": [],
      "source": [
        "def load_mnist_bypath(paths, verbose=-1):\n",
        "    \"\"\" Expect to read images where each class is in a separate directory,\n",
        "        For example: images of type 0 are in folder 0\n",
        "    \"\"\"\n",
        "\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2]\n",
        "        # here the img is scaled to [0, 1] to less the impact of each pixel's brightness\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
        "            print(f\"[INFO] processed {i+1}/{len(paths)}\")\n",
        "\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8GgAuQUTi42",
        "outputId": "831655d7-1881-4f0b-af32-8b086b90c6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] processed 10000/42000\n",
            "[INFO] processed 20000/42000\n",
            "[INFO] processed 30000/42000\n",
            "[INFO] processed 40000/42000\n"
          ]
        }
      ],
      "source": [
        "# Declare mnist dataset path\n",
        "# img_path = \"mnist/trainingSet/trainingSet\"\n",
        "img_path = \"/content/mnist/trainingSet/trainingSet\"\n",
        "\n",
        "# Generate a list of trucks using the list_images function from the paths library\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "# load images into arrays\n",
        "image_list, label_list = load_mnist_bypath(image_paths, verbose=10000)\n",
        "\n",
        "# Perform the one-hot encoded so we can use the sparse-categorical-entropy loss function\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4LqxZDHMTi43"
      },
      "outputs": [],
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial=\"clients\"):\n",
        "    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n",
        "                will be the data fragment - tuple of images and labels.\n",
        "        args:\n",
        "            image_list: a numpy array object with the images\n",
        "            label_list: list of binarized labels (one-hot encoded)\n",
        "            num_client: number of customers (clients)\n",
        "            initials: the prefix of the clients, e.g., clients_1\n",
        "     \"\"\"\n",
        "\n",
        "    # create list of customer names\n",
        "    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n",
        "\n",
        "    # shuffle the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # shard the data and split it for each customer\n",
        "    size = len(data) // num_clients\n",
        "    shards = [data[i: i+size] for i in range(0, size * num_clients, size)]\n",
        "\n",
        "    # Check if the fragment number is equal to the number of clients\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
        "\n",
        "\n",
        "# Create the customers\n",
        "clients = create_clients(X_train, y_train, num_clients=100, initial=\"client\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_xXnY6tFTi43"
      },
      "outputs": [],
      "source": [
        "def batch_data(data_shard, b=32):\n",
        "    \"\"\" Receives a piece of data from a client and creates a tensorflow data object in it\n",
        "        args:\n",
        "            data_shard: data and labels that make up a customer's data shard\n",
        "            b: batch size\n",
        "        return:\n",
        "            data tensorflow object\n",
        "    \"\"\"\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(b)\n",
        "\n",
        "\n",
        "# Process and collate the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "# process and group test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xe1ZFi2gTi44"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(100, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(100))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "\n",
        "        return model\n",
        "\n",
        "lr = 0.01\n",
        "comms_round = 30\n",
        "loss = \"categorical_crossentropy\"\n",
        "metrics = [\"accuracy\"]\n",
        "# optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)\n",
        "# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=lr/comms_round, momentum=0.9)\n",
        "\n",
        "# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LiDTqgXgTi44"
      },
      "outputs": [],
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name, participants):\n",
        "    \"\"\" Calculates the size ratio of a client's local training data\n",
        "        with all general training data maintained by all customers\n",
        "    \"\"\"\n",
        "    # client_names = list(clients_trn_data.keys())\n",
        "    # calculate batch size\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    # first calculate total training data across clients\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()\n",
        "                        for client_name in participants]) * bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * bs\n",
        "\n",
        "    return local_count / global_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7wFLVmqtTi44"
      },
      "outputs": [],
      "source": [
        "def scale_model_weights(weight, scalar):\n",
        "    \"\"\" Scale the model weights \"\"\"\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "\n",
        "    return weight_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uTH6M7XCTi45"
      },
      "outputs": [],
      "source": [
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    \"\"\" Return the sum of the listed scaled weights. O is equivalent to the average weight of the weights \"\"\"\n",
        "    avg_grad = list()\n",
        "    # get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "JJNKf44FTi45"
      },
      "outputs": [],
      "source": [
        "def test_model(X_test, Y_test, model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    # logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    # print(f\"Agregation Round: {comm_round} | global_acc: {acc:.3%} | global_loss: {loss}\"\")\n",
        "    print(f\"round: {comm_round} | acc: {acc:.3%} | loss: {loss}\")\n",
        "\n",
        "    return acc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k8wrKPisTi45"
      },
      "outputs": [],
      "source": [
        "def check_local_loss(client, model):\n",
        "    # Check local loss\n",
        "    cce_l = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    client_x = np.array([i[0] for i in clients[client]])\n",
        "    client_y = np.array([i[1] for i in clients[client]])\n",
        "    logits_l = model.predict(client_x)\n",
        "    loss_l = cce_l(client_y, logits_l)\n",
        "    acc_l = accuracy_score(tf.argmax(logits_l, axis=1), tf.argmax(client_y, axis=1))\n",
        "    print(f\"Local accuracy: {acc_l}. Local loss: {loss_l}\")\n",
        "\n",
        "    return acc_l, loss_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZBZ4LjTTi45",
        "outputId": "d0e7901a-d638-4f40-fe7f-a9369b4e07f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 0 | acc: 39.198% | loss: 2.2342634201049805\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 1 | acc: 75.595% | loss: 1.8431953191757202\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 2 | acc: 82.825% | loss: 1.7698490619659424\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 3 | acc: 86.373% | loss: 1.680792212486267\n",
            "394/394 [==============================] - 1s 3ms/step\n",
            "round: 4 | acc: 87.437% | loss: 1.648942232131958\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 5 | acc: 88.579% | loss: 1.6215823888778687\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 6 | acc: 89.024% | loss: 1.6178090572357178\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 7 | acc: 88.786% | loss: 1.6079368591308594\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 8 | acc: 90.563% | loss: 1.590053677558899\n",
            "394/394 [==============================] - 2s 4ms/step\n",
            "round: 9 | acc: 91.032% | loss: 1.581508755683899\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 10 | acc: 90.952% | loss: 1.5770699977874756\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 11 | acc: 91.516% | loss: 1.572814702987671\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 12 | acc: 91.873% | loss: 1.5680420398712158\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 13 | acc: 91.762% | loss: 1.5673117637634277\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 14 | acc: 92.667% | loss: 1.5610440969467163\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 15 | acc: 92.556% | loss: 1.558719277381897\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 16 | acc: 92.873% | loss: 1.5556495189666748\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 17 | acc: 93.206% | loss: 1.5488821268081665\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 18 | acc: 93.310% | loss: 1.5479695796966553\n",
            "394/394 [==============================] - 1s 3ms/step\n",
            "round: 19 | acc: 93.333% | loss: 1.5483678579330444\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 20 | acc: 93.508% | loss: 1.546379566192627\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 21 | acc: 93.770% | loss: 1.5436320304870605\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 22 | acc: 93.706% | loss: 1.5439397096633911\n",
            "394/394 [==============================] - 1s 3ms/step\n",
            "round: 23 | acc: 93.690% | loss: 1.5422016382217407\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 24 | acc: 94.032% | loss: 1.5456551313400269\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 25 | acc: 93.833% | loss: 1.5377204418182373\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 26 | acc: 94.167% | loss: 1.5451148748397827\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 27 | acc: 94.135% | loss: 1.5379067659378052\n",
            "394/394 [==============================] - 1s 2ms/step\n",
            "round: 28 | acc: 94.032% | loss: 1.5442993640899658\n",
            "394/394 [==============================] - 1s 3ms/step\n",
            "round: 29 | acc: 94.238% | loss: 1.5383460521697998\n"
          ]
        }
      ],
      "source": [
        "### Start global template ###\n",
        "\n",
        "smlp_global = MLP()\n",
        "global_model = smlp_global.build(784, 10)\n",
        "\n",
        "# Global training loop collection\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    # randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "    client_select = client_names[0:55]\n",
        "    # print(client_select)\n",
        "\n",
        "    # loop through each client and create a new local model\n",
        "    for client in client_select:\n",
        "        smlp_local = MLP()\n",
        "        local_model = smlp_local.build(784, 10)\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "        # set the weight of the local model to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        # fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
        "\n",
        "        # scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client, client_select)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        # Check local accuracy\n",
        "        # acc_l, loss_l = check_local_loss(client, local_model)\n",
        "\n",
        "        # clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    # to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # update global model\n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    # test global model and print out metrics after each communications round\n",
        "    for (X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vRTxww2PTi46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K-jfRsbtTi46"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
