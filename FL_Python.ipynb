{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hMM6D1tDTi4y"
      },
      "outputs": [],
      "source": [
        "# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=122af6eb9e746e2774d3422cba7775dd9ca2a43aaa3156f4c0b5043723a24823\"\n",
        "# name = url.split(\"/\")[-1]\n",
        "# fileName = name.split(\"?\")[0]\n",
        "\n",
        "# !wget $url\n",
        "# !mv $name $fileName"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sGtX5o7VTi41"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !unzip mnist.zip;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RzTrKyXWTi41"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Dense\n",
        "from keras import optimizers\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "mySeed = 42\n",
        "np.random.seed(mySeed)\n",
        "random.seed(mySeed)\n",
        "tf.random.set_seed(mySeed)\n",
        "# torch.manual_seed(mySeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XpvfFXbOTi42"
      },
      "outputs": [],
      "source": [
        "def load_mnist_bypath(paths, verbose=-1):\n",
        "    \"\"\" Expect to read images where each class is in a separate directory,\n",
        "        For example: images of type 0 are in folder 0\n",
        "    \"\"\"\n",
        "\n",
        "    data = list()\n",
        "    labels = list()\n",
        "    # loop over the input images\n",
        "    for (i, imgpath) in enumerate(paths):\n",
        "        # load the image and extract the class labels\n",
        "        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n",
        "        image = np.array(im_gray).flatten()\n",
        "        label = imgpath.split(os.path.sep)[-2]\n",
        "        # here the img is scaled to [0, 1] to less the impact of each pixel's brightness\n",
        "        data.append(image/255)\n",
        "        labels.append(label)\n",
        "        # show an update every `verbose` images\n",
        "        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
        "            print(f\"[INFO] processed {i+1}/{len(paths)}\")\n",
        "            break\n",
        "\n",
        "    # return a tuple of the data and labels\n",
        "    return data, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8GgAuQUTi42",
        "outputId": "831655d7-1881-4f0b-af32-8b086b90c6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] processed 10000/42000\n"
          ]
        }
      ],
      "source": [
        "# Declare mnist dataset path\n",
        "img_path = \"mnist/trainingSet/trainingSet\"\n",
        "# img_path = \"/content/mnist/trainingSet/trainingSet\"\n",
        "\n",
        "# Generate a list of trucks using the list_images function from the paths library\n",
        "image_paths = list(paths.list_images(img_path))\n",
        "\n",
        "# load images into arrays\n",
        "image_list, label_list = load_mnist_bypath(image_paths, verbose=10000)\n",
        "\n",
        "# Perform the one-hot encoded so we can use the sparse-categorical-entropy loss function\n",
        "lb = LabelBinarizer()\n",
        "label_list = lb.fit_transform(label_list)\n",
        "\n",
        "#split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4LqxZDHMTi43"
      },
      "outputs": [],
      "source": [
        "def create_clients(image_list, label_list, num_clients=10, initial=\"clients\"):\n",
        "    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n",
        "                will be the data fragment - tuple of images and labels.\n",
        "        args:\n",
        "            image_list: a numpy array object with the images\n",
        "            label_list: list of binarized labels (one-hot encoded)\n",
        "            num_client: number of customers (clients)\n",
        "            initials: the prefix of the clients, e.g., clients_1\n",
        "     \"\"\"\n",
        "\n",
        "    # create list of customer names\n",
        "    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n",
        "\n",
        "    # shuffle the data\n",
        "    data = list(zip(image_list, label_list))\n",
        "    random.shuffle(data)\n",
        "\n",
        "    # shard the data and split it for each customer\n",
        "    size = len(data) // num_clients\n",
        "    shards = [data[i: i+size] for i in range(0, size * num_clients, size)]\n",
        "\n",
        "    # Check if the fragment number is equal to the number of clients\n",
        "    assert(len(shards) == len(client_names))\n",
        "\n",
        "    return {client_names[i]: shards[i] for i in range(len(client_names))}\n",
        "\n",
        "\n",
        "num_clients = 2\n",
        "# Create the customers\n",
        "clients = create_clients(X_train, y_train, num_clients=num_clients, initial=\"client\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_xXnY6tFTi43"
      },
      "outputs": [],
      "source": [
        "def batch_data(data_shard, b=32):\n",
        "    \"\"\" Receives a piece of data from a client and creates a tensorflow data object in it\n",
        "        args:\n",
        "            data_shard: data and labels that make up a customer's data shard\n",
        "            b: batch size\n",
        "        return:\n",
        "            data tensorflow object\n",
        "    \"\"\"\n",
        "    #seperate shard into data and labels lists\n",
        "    data, label = zip(*data_shard)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "    return dataset.shuffle(len(label)).batch(b)\n",
        "\n",
        "\n",
        "# Process and collate the training data for each client\n",
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "\n",
        "# process and group test set\n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[([1, 2], 'a'), ([3, 4], 'b'), ([5, 6], 'c'), ([7, 8], 'd')]\n",
            "([1, 2], [3, 4], [5, 6], [7, 8])\n",
            "('a', 'b', 'c', 'd')\n",
            "([[1, 2], [3, 4], [5, 6], [7, 8]], ['a', 'b', 'c', 'd']) \n",
            "\n",
            "[(array([5, 6]), b'c'), (array([3, 4]), b'b'), (array([7, 8]), b'd'), (array([1, 2]), b'a')]\n",
            "[(array([[1, 2],\n",
            "       [5, 6],\n",
            "       [3, 4],\n",
            "       [7, 8]]), array([b'a', b'c', b'b', b'd'], dtype=object))]\n",
            "---------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int64, numpy=4>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test = list(zip([[1, 2], [3, 4], [5, 6], [7, 8]], [\"a\", \"b\", \"c\", \"d\"]))\n",
        "print(test)\n",
        "data, label = zip(*test)\n",
        "print(data)\n",
        "print(label)\n",
        "\n",
        "print((list(data), list(label)), \"\\n\")\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
        "test1 = dataset.shuffle(len(label))\n",
        "print(list(test1.as_numpy_iterator()))\n",
        "test2 = dataset.shuffle(len(label)).batch(4)\n",
        "print(list(test2.as_numpy_iterator()))\n",
        "\n",
        "print(\"---------\")\n",
        "list(test2)[0][0].shape[0]\n",
        "\n",
        "tf.data.experimental.cardinality(test1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xe1ZFi2gTi44"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    @staticmethod\n",
        "    def build(shape, classes):\n",
        "        model = Sequential()\n",
        "        model.add(Dense(100, input_shape=(shape,)))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(100))\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Dense(classes))\n",
        "        model.add(Activation(\"softmax\"))\n",
        "\n",
        "        return model\n",
        "\n",
        "lr = 0.01\n",
        "comms_round = 1\n",
        "loss = \"categorical_crossentropy\"\n",
        "metrics = [\"accuracy\"]\n",
        "# optimizer = SGD(lr=lr, decay=lr/comms_round, momentum=0.9)\n",
        "# optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, decay=lr/comms_round, momentum=0.9)\n",
        "\n",
        "# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LiDTqgXgTi44"
      },
      "outputs": [],
      "source": [
        "def weight_scalling_factor(clients_trn_data, client_name, participants):\n",
        "    \"\"\" Calculates the size ratio of a client's local training data\n",
        "        with all general training data maintained by all customers\n",
        "    \"\"\"\n",
        "    # client_names = list(clients_trn_data.keys())\n",
        "    # calculate batch size\n",
        "    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n",
        "    # first calculate total training data across clients\n",
        "    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()\n",
        "                        for client_name in participants]) * bs\n",
        "    # get the total number of data points held by a client\n",
        "    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() * bs\n",
        "\n",
        "    return local_count / global_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7wFLVmqtTi44"
      },
      "outputs": [],
      "source": [
        "def scale_model_weights(weight, scalar):\n",
        "    \"\"\" Scale the model weights \"\"\"\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "        weight_final.append(scalar * weight[i])\n",
        "\n",
        "    return weight_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uTH6M7XCTi45"
      },
      "outputs": [],
      "source": [
        "def sum_scaled_weights(scaled_weight_list):\n",
        "    \"\"\" Return the sum of the listed scaled weights. O is equivalent to the average weight of the weights \"\"\"\n",
        "    avg_grad = list()\n",
        "    # get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "\n",
        "    return avg_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JJNKf44FTi45"
      },
      "outputs": [],
      "source": [
        "def test_model(X_test, Y_test, model, comm_round):\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    # logits = model.predict(X_test, batch_size=100)\n",
        "    logits = model.predict(X_test)\n",
        "    loss = cce(Y_test, logits)\n",
        "    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n",
        "    # print(f\"Agregation Round: {comm_round} | global_acc: {acc:.3%} | global_loss: {loss}\"\")\n",
        "    print(f\"round: {comm_round} | acc: {acc:.3%} | loss: {loss}\")\n",
        "\n",
        "    return acc, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k8wrKPisTi45"
      },
      "outputs": [],
      "source": [
        "def check_local_loss(client, model):\n",
        "    # Check local loss\n",
        "    cce_l = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "    client_x = np.array([i[0] for i in clients[client]])\n",
        "    client_y = np.array([i[1] for i in clients[client]])\n",
        "    logits_l = model.predict(client_x, verbose=0)\n",
        "    loss_l = cce_l(client_y, logits_l)\n",
        "    acc_l = accuracy_score(tf.argmax(logits_l, axis=1), tf.argmax(client_y, axis=1))\n",
        "    print(f\"Local accuracy: {acc_l}. Local loss: {loss_l}\")\n",
        "\n",
        "    return acc_l, loss_l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZBZ4LjTTi45",
        "outputId": "d0e7901a-d638-4f40-fe7f-a9369b4e07f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Local accuracy: 0.9925. Local loss: 0.024987656623125076\n",
            "Local accuracy: 0.996. Local loss: 0.01440570317208767\n"
          ]
        }
      ],
      "source": [
        "client_select_rate = 0.6\n",
        "client_select_rate = 1\n",
        "client_epochs = 1\n",
        "### Start global template ###\n",
        "\n",
        "smlp_global = MLP()\n",
        "global_model = smlp_global.build(784, np.array(y_train).shape[-1])\n",
        "\n",
        "# Global training loop collection\n",
        "for comm_round in range(comms_round):\n",
        "\n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "\n",
        "    # initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    # randomize client data - using keys\n",
        "    client_names = list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "    client_select = client_names[0: int(num_clients*client_select_rate)]\n",
        "\n",
        "    # loop through each client and create a new local model\n",
        "    for client in client_select:\n",
        "        smlp_local = MLP()\n",
        "        local_model = smlp_local.build(784, np.array(y_train).shape[-1])\n",
        "        local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
        "\n",
        "        # set the weight of the local model to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "\n",
        "        # fit local model with client's data\n",
        "        local_model.fit(clients_batched[client], epochs=client_epochs, verbose=0)\n",
        "\n",
        "        # scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client, client_select)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "\n",
        "        # Check local accuracy\n",
        "        acc_l, loss_l = check_local_loss(client, local_model)\n",
        "\n",
        "        # clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "\n",
        "    # # to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    # average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "\n",
        "    # # update global model\n",
        "    # global_model.set_weights(average_weights)\n",
        "\n",
        "    # # test global model and print out metrics after each communications round\n",
        "    # for (X_test, Y_test) in test_batched:\n",
        "    #     global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-jfRsbtTi46"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
