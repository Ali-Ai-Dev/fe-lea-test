{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive/ColabTemp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeed = 42\n",
    "torch.manual_seed(mySeed)\n",
    "np.random.seed(mySeed)\n",
    "random.seed(mySeed)\n",
    "# tf.random.set_seed(mySeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_clients = 3\n",
    "batch_size = 1000\n",
    "total_steps = 2\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = 2\n",
    "\n",
    "remain = 0.1 # Remove some data for running faster in test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "torch.Size([3, 32, 32])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "# Remove some data for running faster in test\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, int(len(train_data)*remain)))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random dataset split\n",
    "client_data_size = np.array([len(train_data)//num_clients]*num_clients)\n",
    "data_remain = len(train_data) % num_clients\n",
    "for i in range(data_remain):\n",
    "    client_data_size[-1-i] += 1\n",
    "\n",
    "client_datasets = torch.utils.data.random_split(train_data, client_data_size)\n",
    "\n",
    "### None random dataset split\n",
    "# client_datasets = list()\n",
    "# i = 0\n",
    "# for j in client_data_size:\n",
    "#     client_datasets.append(torch.utils.data.Subset(train_data, range(i, i+j)))\n",
    "#     i += j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for each client\n",
    "client_dataloaders = np.zeros(num_clients, dtype=object)\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    client_dataloaders[i] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "input_flat_size = torch.flatten(train_data[0][0]).shape[0]\n",
    "nClasses = train_data[0][1].shape[0]\n",
    "\n",
    "class NeuralNetworkMnistMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 256)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(256, 128)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(128, 64)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(64, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkMnistMLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (fc1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (relu3): ReLU()\n",
      "    (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "global_model = NeuralNetworkMnistMLP().to(device)\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights = global_model.get_weights()\n",
    "client_selects = torch.arange(0, num_clients)\n",
    "client_weights = [copy.deepcopy(global_weights)  for _ in range(len(client_selects))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print_step = np.ceil(len(dataloader)/10)\n",
    "        if batch % print_step == 0:\n",
    "            loss_per_batch = running_loss / print_step\n",
    "            current_item = (batch+1)*len(x)\n",
    "            print(f\"loss: {loss_per_batch:>7f}  [{current_item:>5d}/{data_size:>5d}]\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clinet(dataloader, model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(client_epochs):\n",
    "        train(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.302497  [ 1000/ 1666]\n",
      "loss: 2.310790  [ 1332/ 1666]\n",
      "loss: 2.324357  [ 1000/ 1666]\n",
      "loss: 2.331970  [ 1332/ 1666]\n",
      "loss: 2.302649  [ 1000/ 1667]\n",
      "loss: 2.338618  [ 1334/ 1667]\n",
      "loss: 2.300561  [ 1000/ 1667]\n",
      "loss: 2.322997  [ 1334/ 1667]\n",
      "loss: 2.302762  [ 1000/ 1667]\n",
      "loss: 2.302717  [ 1334/ 1667]\n",
      "loss: 2.345971  [ 1000/ 1667]\n",
      "loss: 2.358479  [ 1334/ 1667]\n",
      "loss: 2.375062  [ 1000/ 1666]\n",
      "loss: 2.358223  [ 1332/ 1666]\n",
      "loss: 2.352713  [ 1000/ 1666]\n",
      "loss: 2.339923  [ 1332/ 1666]\n",
      "loss: 2.348088  [ 1000/ 1667]\n",
      "loss: 2.346797  [ 1334/ 1667]\n",
      "loss: 2.364607  [ 1000/ 1667]\n",
      "loss: 2.346546  [ 1334/ 1667]\n",
      "loss: 2.351098  [ 1000/ 1667]\n",
      "loss: 2.362106  [ 1334/ 1667]\n",
      "loss: 2.358114  [ 1000/ 1667]\n",
      "loss: 2.342354  [ 1334/ 1667]\n"
     ]
    }
   ],
   "source": [
    "for step in range(0, total_steps):\n",
    "    for client in client_selects:\n",
    "        local_model = NeuralNetworkMnistMLP().to(device)\n",
    "        local_model.set_weights(client_weights[client])\n",
    "        train_clinet(client_dataloaders[client], local_model)\n",
    "        client_weights[client] = local_model.get_weights()\n",
    "\n",
    "        del local_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0056,  0.0065, -0.0187,  ...,  0.0074, -0.0216, -0.0271],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0081,  0.0240, -0.0023,  ..., -0.0110, -0.0406, -0.0453],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0030,  0.0145, -0.0104,  ...,  0.0154, -0.0109, -0.0206],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(client_weights[0][0][0])\n",
    "print(client_weights[1][0][0])\n",
    "print(client_weights[2][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
