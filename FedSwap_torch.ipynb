{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive/ColabTemp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeed = 42\n",
    "torch.manual_seed(mySeed)\n",
    "np.random.seed(mySeed)\n",
    "random.seed(mySeed)\n",
    "# tf.random.set_seed(mySeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_clients = 3\n",
    "batch_size = 1000\n",
    "total_steps = 2\n",
    "client_select_percentage = 1\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = 2\n",
    "\n",
    "swap_step = 2\n",
    "n_swap_bet_avg_p1 = 3 # p1=plus one to your number, if need 2 swap between avg, enter 3\n",
    "\n",
    "remain = 1 # Remove some data for running faster in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_selects = None\n",
    "client_weights = None"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "torch.Size([3, 32, 32])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Remove some data for running faster in test\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, int(len(train_data)*remain)))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random dataset split\n",
    "client_data_size = np.array([len(train_data)//num_clients]*num_clients)\n",
    "data_remain = len(train_data) % num_clients\n",
    "for i in range(data_remain):\n",
    "    client_data_size[-1-i] += 1\n",
    "\n",
    "client_datasets = torch.utils.data.random_split(train_data, client_data_size)\n",
    "\n",
    "### None random dataset split\n",
    "# client_datasets = list()\n",
    "# i = 0\n",
    "# for j in client_data_size:\n",
    "#     client_datasets.append(torch.utils.data.Subset(train_data, range(i, i+j)))\n",
    "#     i += j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for each client\n",
    "client_dataloaders = np.zeros(num_clients, dtype=object)\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    client_dataloaders[i] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "input_flat_size = torch.flatten(train_data[0][0]).shape[0]\n",
    "nClasses = train_data[0][1].shape[0]\n",
    "\n",
    "class NeuralNetworkMnistMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 256)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(256, 128)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(128, 64)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(64, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkMnistMLP(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (fc1): Linear(in_features=3072, out_features=256, bias=True)\n",
      "    (relu1): ReLU()\n",
      "    (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (relu2): ReLU()\n",
      "    (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (relu3): ReLU()\n",
      "    (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "global_model = NeuralNetworkMnistMLP().to(device)\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clients_and_assign_weights(global_weights):\n",
    "    global client_selects\n",
    "    global client_weights\n",
    "\n",
    "    lst = np.arange(0, num_clients)\n",
    "    np.random.shuffle(lst)\n",
    "    client_selects = lst[: int(len(lst)*client_select_percentage)]\n",
    "\n",
    "    client_weights = {i: copy.deepcopy(global_weights)  for i in client_selects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights = global_model.get_weights()\n",
    "select_clients_and_assign_weights(global_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weights, scalar):\n",
    "    \"\"\" Scale the model weights \"\"\"\n",
    "\n",
    "    scaled_weights = list()\n",
    "    for i in range(len(weights)):\n",
    "        scaled_weights.append(weights[i] * scalar)\n",
    "\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(client_scaled_weights):\n",
    "    \"\"\" Return the sum of the listed scaled weights.\n",
    "        axis_O is equivalent to the average weight of the weights \"\"\"\n",
    "\n",
    "    avg_weights = list()\n",
    "    # get the average gradient accross all client gradients\n",
    "    for gradient_list_tuple in zip(*client_scaled_weights):\n",
    "        gradient_list_tuple = [tensor.tolist()  for tensor in gradient_list_tuple]\n",
    "        layer_mean = torch.sum(torch.tensor(gradient_list_tuple), axis=0).to(device)\n",
    "        avg_weights.append(layer_mean)\n",
    "\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "### Explaining the function with example ###\n",
    "# t = (torch.tensor([[[2, 3],[3, 4]], [[3, 4],[4, 5]], [[4, 5],[5, 6]]]),\n",
    "#      torch.tensor([[[5, 6],[6, 7]], [[6, 7],[7, 8]], [[7, 8],[8, 9]]]))\n",
    "# t = [i.tolist() for i in t]\n",
    "# for y in zip(*t):\n",
    "#     print(y)\n",
    "#     print(torch.sum(torch.tensor(y), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg():\n",
    "    # calculate total training data across clients\n",
    "    global_count = 0\n",
    "    for client in client_selects:\n",
    "        global_count += len(client_dataloaders[client].dataset)\n",
    "\n",
    "    # initial list to collect clients weight after scalling\n",
    "    client_scaled_weights = list()\n",
    "    for client in client_selects:\n",
    "        local_count = len(client_dataloaders[client].dataset)\n",
    "        scaling_factor = local_count / global_count\n",
    "        scaled_weights = scale_model_weights(client_weights[client], scaling_factor)\n",
    "        client_scaled_weights.append(scaled_weights)\n",
    "\n",
    "    # to get the average over all the clients model, we simply take the sum of the scaled weights\n",
    "    avg_weights = sum_scaled_weights(client_scaled_weights)\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct_items = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_items += (pred.argmax(1) == y.argmax(1)).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / num_batches\n",
    "    accuracy = correct_items / data_size\n",
    "    print(f\"Test Error: \\nAccuracy: {(accuracy*100):>0.1f}%, Loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print_step = np.ceil(len(dataloader)/10)\n",
    "        if batch % print_step == 0:\n",
    "            loss_per_batch = running_loss / print_step\n",
    "            current_item = (batch+1)*len(x)\n",
    "            print(f\"loss: {loss_per_batch:>7f}  [{current_item:>5d}/{data_size:>5d}]\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clinet(dataloader, model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(client_epochs):\n",
    "        train_neural_network(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.151384  [ 1000/16666]\n",
      "loss: 2.323791  [ 3000/16666]\n",
      "loss: 2.349789  [ 5000/16666]\n",
      "loss: 2.366760  [ 7000/16666]\n",
      "loss: 2.351067  [ 9000/16666]\n",
      "loss: 2.369150  [11000/16666]\n",
      "loss: 2.363650  [13000/16666]\n",
      "loss: 2.365150  [15000/16666]\n",
      "loss: 2.351346  [11322/16666]\n",
      "loss: 1.188575  [ 1000/16666]\n",
      "loss: 2.371650  [ 3000/16666]\n",
      "loss: 2.357650  [ 5000/16666]\n",
      "loss: 2.360150  [ 7000/16666]\n",
      "loss: 2.357650  [ 9000/16666]\n",
      "loss: 2.363650  [11000/16666]\n",
      "loss: 2.351650  [13000/16666]\n",
      "loss: 2.363150  [15000/16666]\n",
      "loss: 2.368602  [11322/16666]\n",
      "loss: 1.151338  [ 1000/16667]\n",
      "loss: 2.325502  [ 3000/16667]\n",
      "loss: 2.338013  [ 5000/16667]\n",
      "loss: 2.349757  [ 7000/16667]\n",
      "loss: 2.367121  [ 9000/16667]\n",
      "loss: 2.353091  [11000/16667]\n",
      "loss: 2.335261  [13000/16667]\n",
      "loss: 2.337777  [15000/16667]\n",
      "loss: 2.356467  [11339/16667]\n",
      "loss: 1.187067  [ 1000/16667]\n",
      "loss: 2.375157  [ 3000/16667]\n",
      "loss: 2.367133  [ 5000/16667]\n",
      "loss: 2.352149  [ 7000/16667]\n",
      "loss: 2.365149  [ 9000/16667]\n",
      "loss: 2.366650  [11000/16667]\n",
      "loss: 2.354649  [13000/16667]\n",
      "loss: 2.374649  [15000/16667]\n",
      "loss: 2.364425  [11339/16667]\n",
      "loss: 1.151329  [ 1000/16667]\n",
      "loss: 2.321782  [ 3000/16667]\n",
      "loss: 2.348084  [ 5000/16667]\n",
      "loss: 2.349115  [ 7000/16667]\n",
      "loss: 2.365729  [ 9000/16667]\n",
      "loss: 2.361150  [11000/16667]\n",
      "loss: 2.366150  [13000/16667]\n",
      "loss: 2.356150  [15000/16667]\n",
      "loss: 2.352926  [11339/16667]\n",
      "loss: 1.179075  [ 1000/16667]\n",
      "loss: 2.368650  [ 3000/16667]\n",
      "loss: 2.346650  [ 5000/16667]\n",
      "loss: 2.359150  [ 7000/16667]\n",
      "loss: 2.360150  [ 9000/16667]\n",
      "loss: 2.362650  [11000/16667]\n",
      "loss: 2.367650  [13000/16667]\n",
      "loss: 2.354150  [15000/16667]\n",
      "loss: 2.366675  [11339/16667]\n",
      "Test Error: \n",
      "Accuracy: 10.0%, Loss: 2.360817\n",
      "\n",
      "loss: 1.186871  [ 1000/16667]\n",
      "loss: 2.361651  [ 3000/16667]\n",
      "loss: 2.369650  [ 5000/16667]\n",
      "loss: 2.370150  [ 7000/16667]\n",
      "loss: 2.379650  [ 9000/16667]\n",
      "loss: 2.350150  [11000/16667]\n",
      "loss: 2.356150  [13000/16667]\n",
      "loss: 2.372651  [15000/16667]\n",
      "loss: 2.365676  [11339/16667]\n",
      "loss: 1.180575  [ 1000/16667]\n",
      "loss: 2.370650  [ 3000/16667]\n",
      "loss: 2.370150  [ 5000/16667]\n",
      "loss: 2.371150  [ 7000/16667]\n",
      "loss: 2.357647  [ 9000/16667]\n",
      "loss: 2.373653  [11000/16667]\n",
      "loss: 2.366150  [13000/16667]\n",
      "loss: 2.346650  [15000/16667]\n",
      "loss: 2.352180  [11339/16667]\n",
      "loss: 1.179877  [ 1000/16667]\n",
      "loss: 2.350653  [ 3000/16667]\n",
      "loss: 2.359150  [ 5000/16667]\n",
      "loss: 2.357650  [ 7000/16667]\n",
      "loss: 2.365150  [ 9000/16667]\n",
      "loss: 2.366650  [11000/16667]\n",
      "loss: 2.359718  [13000/16667]\n",
      "loss: 2.360150  [15000/16667]\n",
      "loss: 2.363674  [11339/16667]\n",
      "loss: 1.187575  [ 1000/16667]\n",
      "loss: 2.375650  [ 3000/16667]\n",
      "loss: 2.365150  [ 5000/16667]\n",
      "loss: 2.353650  [ 7000/16667]\n",
      "loss: 2.356150  [ 9000/16667]\n",
      "loss: 2.359150  [11000/16667]\n",
      "loss: 2.344650  [13000/16667]\n",
      "loss: 2.359650  [15000/16667]\n",
      "loss: 2.355431  [11339/16667]\n",
      "loss: 1.183950  [ 1000/16666]\n",
      "loss: 2.360654  [ 3000/16666]\n",
      "loss: 2.360150  [ 5000/16666]\n",
      "loss: 2.351650  [ 7000/16666]\n",
      "loss: 2.362149  [ 9000/16666]\n",
      "loss: 2.354143  [11000/16666]\n",
      "loss: 2.351953  [13000/16666]\n",
      "loss: 2.358650  [15000/16666]\n",
      "loss: 2.348587  [11322/16666]\n",
      "loss: 1.168074  [ 1000/16666]\n",
      "loss: 2.364149  [ 3000/16666]\n",
      "loss: 2.363606  [ 5000/16666]\n",
      "loss: 2.361310  [ 7000/16666]\n",
      "loss: 2.364047  [ 9000/16666]\n",
      "loss: 2.356648  [11000/16666]\n",
      "loss: 2.368147  [13000/16666]\n",
      "loss: 2.372150  [15000/16666]\n",
      "loss: 2.360279  [11322/16666]\n"
     ]
    }
   ],
   "source": [
    "for step in range(0, total_steps):\n",
    "    for client in client_selects:\n",
    "        local_model = NeuralNetworkMnistMLP().to(device)\n",
    "        local_model.set_weights(client_weights[client])\n",
    "        train_clinet(client_dataloaders[client], local_model)\n",
    "        client_weights[client] = local_model.get_weights()\n",
    "\n",
    "        del local_model\n",
    "    \n",
    "    if (step % swap_step == 0) and (step % (swap_step*n_swap_bet_avg_p1) != 0):\n",
    "        pass\n",
    "    \n",
    "    if (step % (swap_step*n_swap_bet_avg_p1) == 0):\n",
    "        avg_weights = fed_avg()\n",
    "        global_model.set_weights(avg_weights) # update global model\n",
    "        select_clients_and_assign_weights(avg_weights)\n",
    "\n",
    "        # test global model and print out metrics after each communication round\n",
    "        global_acc, global_loss = test_neural_network(test_dataloader, global_model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0501, -0.0390, -0.0499,  ..., -0.0143, -0.0179, -0.0232],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0305, -0.0242, -0.0409,  ..., -0.0089, -0.0124, -0.0168],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "tensor([-0.0155, -0.0046, -0.0155,  ...,  0.0180,  0.0144,  0.0096],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(client_weights[0][0][0])\n",
    "print(client_weights[1][0][0])\n",
    "print(client_weights[2][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
