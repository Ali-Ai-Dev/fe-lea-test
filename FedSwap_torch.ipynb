{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd \"/content/drive/MyDrive/ColabTemp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeed = 42\n",
    "torch.manual_seed(mySeed)\n",
    "np.random.seed(mySeed)\n",
    "random.seed(mySeed)\n",
    "# tf.random.set_seed(mySeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_clients = 3\n",
    "batch_size = 64\n",
    "total_steps = 10\n",
    "client_select_percentage = 1\n",
    "\n",
    "learning_rate = 0.001\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = 2\n",
    "\n",
    "swap_step = 2\n",
    "n_swap_bet_avg_p1 = 2 # p1=plus one to your number, if need 2 swap between avg, enter 3\n",
    "\n",
    "remain = 1 # Remove some data for running faster in test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_selects = None\n",
    "client_weights = None\n",
    "\n",
    "is_print_eval = False\n",
    "start_bold = \"\\033[1m\"\n",
    "end_bold = \"\\033[0;0m\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "50000\n",
      "torch.Size([3, 32, 32])\n",
      "tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "train_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"../datasets\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    ")\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data[0][0].shape)\n",
    "print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Remove some data for running faster in test\n",
    "print(len(train_data))\n",
    "train_data = torch.utils.data.Subset(train_data, range(0, int(len(train_data)*remain)))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random dataset split\n",
    "client_data_size = np.array([len(train_data)//num_clients]*num_clients)\n",
    "data_remain = len(train_data) % num_clients\n",
    "for i in range(data_remain):\n",
    "    client_data_size[-1-i] += 1\n",
    "\n",
    "client_datasets = torch.utils.data.random_split(train_data, client_data_size)\n",
    "\n",
    "### None random dataset split\n",
    "# client_datasets = list()\n",
    "# i = 0\n",
    "# for j in client_data_size:\n",
    "#     client_datasets.append(torch.utils.data.Subset(train_data, range(i, i+j)))\n",
    "#     i += j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for each client\n",
    "client_dataloaders = np.zeros(num_clients, dtype=object)\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    client_dataloaders[i] = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_conv_max_layers(in_w, in_h, kernels, strides, paddings=None, dilations=None):\n",
    "    # In MaxPool2d, strides must same with kernels\n",
    "    if paddings == None:\n",
    "        paddings = np.zeros(len(kernels))\n",
    "    \n",
    "    if dilations == None:\n",
    "        dilations = np.ones(len(kernels))\n",
    "    \n",
    "    out_w = in_w\n",
    "    out_h = in_h\n",
    "    for ker, pad, dil, stri in zip(kernels, paddings, dilations, strides):\n",
    "        out_w = np.floor((out_w + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "        out_h = np.floor((out_h + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "\n",
    "    return int(out_w), int(out_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "input_flat_size = torch.flatten(train_data[0][0]).shape[0]\n",
    "nClasses = train_data[0][1].shape[0]\n",
    "\n",
    "class NeuralNetworkCifar10MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 256)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(256, 128)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(128, 64)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(64, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "\n",
    "input_channels = train_data[0][0].shape[0]\n",
    "conv_l1_out = 32\n",
    "conv_kernel = 3\n",
    "max_kernel = 2\n",
    "kernels = [conv_kernel, max_kernel, conv_kernel, max_kernel]\n",
    "strides = [1, max_kernel, 1, max_kernel]\n",
    "out_w, out_h = calc_out_conv_max_layers(conv_l1_out, conv_l1_out, kernels, strides)\n",
    "\n",
    "class NeuralNetworkCifar10Conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, conv_l1_out, kernel_size=conv_kernel, stride=1, padding=0)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel)),\n",
    "            ('conv2', nn.Conv2d(conv_l1_out, 64, kernel_size=conv_kernel)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w*out_h, 100)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('fc2', nn.Linear(100, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkConv(\n",
      "  (features_stack): Sequential(\n",
      "    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=2304, out_features=100, bias=True)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# global_model = NeuralNetworkCifar10MLP().to(device)\n",
    "global_model = NeuralNetworkCifar10Conv().to(device)\n",
    "print(global_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clients_and_assign_weights(global_weights):\n",
    "    global client_selects\n",
    "    global client_weights\n",
    "\n",
    "    lst = np.arange(0, num_clients)\n",
    "    np.random.shuffle(lst)\n",
    "    client_selects = lst[: int(len(lst)*client_select_percentage)]\n",
    "\n",
    "    client_weights = {i: copy.deepcopy(global_weights)  for i in client_selects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_weights = global_model.get_weights()\n",
    "select_clients_and_assign_weights(global_weights)\n",
    "\n",
    "global_history = {\"times\": {\"train\":list(), \"swap\":list(), \"step\":list()},\n",
    "                  \"accuracy\": list(),\n",
    "                  \"loss\": list()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weights, scalar):\n",
    "    \"\"\" Scale the model weights \"\"\"\n",
    "\n",
    "    scaled_weights = list()\n",
    "    for i in range(len(weights)):\n",
    "        scaled_weights.append(weights[i] * scalar)\n",
    "\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(client_scaled_weights):\n",
    "    \"\"\" Return the sum of the listed scaled weights.\n",
    "        axis_O is equivalent to the average weight of the weights \"\"\"\n",
    "\n",
    "    avg_weights = list()\n",
    "    # get the average gradient accross all client gradients\n",
    "    for gradient_list_tuple in zip(*client_scaled_weights):\n",
    "        gradient_list_tuple = [tensor.tolist()  for tensor in gradient_list_tuple]\n",
    "        layer_mean = torch.sum(torch.tensor(gradient_list_tuple), axis=0).to(device)\n",
    "        avg_weights.append(layer_mean)\n",
    "\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "### Explaining the function with example ###\n",
    "# t = (torch.tensor([[[2, 3],[3, 4]], [[3, 4],[4, 5]], [[4, 5],[5, 6]]]),\n",
    "#      torch.tensor([[[5, 6],[6, 7]], [[6, 7],[7, 8]], [[7, 8],[8, 9]]]))\n",
    "# t = [i.tolist() for i in t]\n",
    "# for y in zip(*t):\n",
    "#     print(y)\n",
    "#     print(torch.sum(torch.tensor(y), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg():\n",
    "    # calculate total training data across clients\n",
    "    global_count = 0\n",
    "    for client in client_selects:\n",
    "        global_count += len(client_dataloaders[client].dataset)\n",
    "\n",
    "    # initial list to collect clients weight after scalling\n",
    "    client_scaled_weights = list()\n",
    "    for client in client_selects:\n",
    "        local_count = len(client_dataloaders[client].dataset)\n",
    "        scaling_factor = local_count / global_count\n",
    "        scaled_weights = scale_model_weights(client_weights[client], scaling_factor)\n",
    "        client_scaled_weights.append(scaled_weights)\n",
    "\n",
    "    # to get the average over all the clients model, we simply take the sum of the scaled weights\n",
    "    avg_weights = sum_scaled_weights(client_scaled_weights)\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_swap(client):\n",
    "    random_num = random.randint(0, len(client_selects)-1)\n",
    "    random_client = client_selects[random_num]\n",
    "\n",
    "    temp_weights = client_weights[random_client]\n",
    "    client_weights[random_client] = client_weights[client]\n",
    "\n",
    "    return temp_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct_items = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_items += (pred.argmax(1) == y.argmax(1)).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / num_batches\n",
    "    accuracy = correct_items / data_size\n",
    "    # print(f\"Test Error: \\nAccuracy: {(accuracy*100):>0.1f}%, Loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print_step = np.ceil(len(dataloader)/10)\n",
    "        if batch % print_step == 0:\n",
    "            loss_per_batch = running_loss / print_step\n",
    "            current_item = (batch+1)*len(x)\n",
    "            # print(f\"loss: {loss_per_batch:>7f}  [{current_item:>5d}/{data_size:>5d}]\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clinet(dataloader, model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(client_epochs):\n",
    "        train_neural_network(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_time_format(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    if h:\n",
    "        return f\"{h:.0f}h-{m:.0f}m-{s:.0f}s\"\n",
    "    elif m:\n",
    "        return f\"{m:.0f}m-{s:.0f}s\"\n",
    "    else:\n",
    "        return f\"{s:.2f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(training_time, swapping_time, step_time, step, metric_index=-1):\n",
    "    training_time = change_time_format(training_time)\n",
    "    swapping_time = change_time_format(swapping_time)\n",
    "    step_time = change_time_format(step_time)\n",
    "    print(f\"round: {step} | training_time: {training_time} | swapping_time: {swapping_time} | step_time: {step_time}\")\n",
    "\n",
    "    global is_print_eval\n",
    "    if is_print_eval:\n",
    "        is_print_eval = False\n",
    "        print(f\"round: {step} / global_acc: {start_bold}{global_history['accuracy'][metric_index]:.4%}{end_bold} / global_loss: {start_bold}{global_history['loss'][metric_index]:.4f}{end_bold}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.151160  [ 1000/16666]\n",
      "loss: 2.328869  [ 3000/16666]\n",
      "loss: 2.300258  [ 5000/16666]\n",
      "loss: 2.296053  [ 7000/16666]\n",
      "loss: 2.279709  [ 9000/16666]\n",
      "loss: 2.251862  [11000/16666]\n",
      "loss: 2.224060  [13000/16666]\n",
      "loss: 2.216454  [15000/16666]\n",
      "loss: 2.223981  [11322/16666]\n",
      "loss: 1.105631  [ 1000/16666]\n",
      "loss: 2.200179  [ 3000/16666]\n",
      "loss: 2.198383  [ 5000/16666]\n",
      "loss: 2.183627  [ 7000/16666]\n",
      "loss: 2.200420  [ 9000/16666]\n",
      "loss: 2.176920  [11000/16666]\n",
      "loss: 2.163659  [13000/16666]\n",
      "loss: 2.160729  [15000/16666]\n",
      "loss: 2.146627  [11322/16666]\n",
      "loss: 1.151361  [ 1000/16667]\n",
      "loss: 2.299383  [ 3000/16667]\n",
      "loss: 2.278898  [ 5000/16667]\n",
      "loss: 2.288036  [ 7000/16667]\n",
      "loss: 2.264523  [ 9000/16667]\n",
      "loss: 2.233517  [11000/16667]\n",
      "loss: 2.231377  [13000/16667]\n",
      "loss: 2.225331  [15000/16667]\n",
      "loss: 2.212402  [11339/16667]\n",
      "loss: 1.106602  [ 1000/16667]\n"
     ]
    }
   ],
   "source": [
    "for step in range(0, total_steps):\n",
    "    training_time_start = time.time()\n",
    "    for client in client_selects:\n",
    "        # local_model = NeuralNetworkCifar10MLP().to(device)\n",
    "        local_model = NeuralNetworkCifar10Conv().to(device)\n",
    "        local_model.set_weights(client_weights[client])\n",
    "        train_clinet(client_dataloaders[client], local_model)\n",
    "        client_weights[client] = local_model.get_weights()\n",
    "\n",
    "        del local_model\n",
    "    \n",
    "    training_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"train\"].append(training_time)\n",
    "\n",
    "\n",
    "    swapping_time_start = time.time()\n",
    "    if (step % swap_step == 0) and (step % (swap_step*n_swap_bet_avg_p1) != 0):\n",
    "        for client in client_selects:\n",
    "            client_weights[client] = fed_swap(client)\n",
    "    \n",
    "    swapping_time = time.time() - swapping_time_start\n",
    "    global_history[\"times\"][\"swap\"].append(swapping_time)\n",
    "    \n",
    "\n",
    "    if (step % (swap_step*n_swap_bet_avg_p1) == 0):\n",
    "        avg_weights = fed_avg()\n",
    "        global_model.set_weights(avg_weights) # update global model\n",
    "        select_clients_and_assign_weights(avg_weights)\n",
    "\n",
    "        is_print_eval = True\n",
    "        # test global model and print out metrics after each communication round\n",
    "        global_acc, global_loss = test_neural_network(test_dataloader, global_model, loss_fn)\n",
    "        global_history[\"accuracy\"].append(global_acc)\n",
    "        global_history[\"loss\"].append(global_loss)\n",
    "    \n",
    "    step_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"step\"].append(step_time)\n",
    "    print_log(training_time, swapping_time, step_time, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0600,  0.0692, -0.2063],\n",
      "         [ 0.0159, -0.0937,  0.0617],\n",
      "         [-0.0744,  0.0236,  0.1050]],\n",
      "\n",
      "        [[-0.1349, -0.2547,  0.1150],\n",
      "         [-0.1693, -0.0140,  0.0597],\n",
      "         [-0.1047,  0.0774,  0.0587]],\n",
      "\n",
      "        [[-0.1031, -0.0083, -0.0446],\n",
      "         [ 0.1298,  0.0762, -0.1789],\n",
      "         [-0.0396,  0.0385,  0.0804]]], device='cuda:0')\n",
      "tensor([[[-0.0600,  0.0692, -0.2063],\n",
      "         [ 0.0159, -0.0937,  0.0617],\n",
      "         [-0.0744,  0.0236,  0.1050]],\n",
      "\n",
      "        [[-0.1349, -0.2547,  0.1150],\n",
      "         [-0.1693, -0.0140,  0.0597],\n",
      "         [-0.1047,  0.0774,  0.0587]],\n",
      "\n",
      "        [[-0.1031, -0.0083, -0.0446],\n",
      "         [ 0.1298,  0.0762, -0.1789],\n",
      "         [-0.0396,  0.0385,  0.0804]]], device='cuda:0')\n",
      "tensor([[[-0.0600,  0.0692, -0.2063],\n",
      "         [ 0.0159, -0.0937,  0.0617],\n",
      "         [-0.0744,  0.0236,  0.1050]],\n",
      "\n",
      "        [[-0.1349, -0.2547,  0.1150],\n",
      "         [-0.1693, -0.0140,  0.0597],\n",
      "         [-0.1047,  0.0774,  0.0587]],\n",
      "\n",
      "        [[-0.1031, -0.0083, -0.0446],\n",
      "         [ 0.1298,  0.0762, -0.1789],\n",
      "         [-0.0396,  0.0385,  0.0804]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(client_weights[0][0][0])\n",
    "print(client_weights[1][0][0])\n",
    "print(client_weights[2][0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
