{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=122af6eb9e746e2774d3422cba7775dd9ca2a43aaa3156f4c0b5043723a24823\"\n",
    "# name = url.split(\"/\")[-1]\n",
    "# fileName = name.split(\"?\")[0]\n",
    "\n",
    "# !wget $url\n",
    "# !mv $name $fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unzip mnist.zip;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imutils import paths\n",
    "import cv2\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import models, layers\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Activation\n",
    "# from keras.layers import Dense\n",
    "# from keras import optimizers\n",
    "# from keras import backend as K"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_bypath(image_paths, verbose=-1):\n",
    "    \"\"\" Expect to read images where each class is in a separate directory,\n",
    "        For example: images of type 0 are in folder 0\n",
    "    \"\"\"\n",
    "\n",
    "    lstData = list()\n",
    "    lstLabel = list()\n",
    "\n",
    "    for (i, imgPath) in enumerate(image_paths):\n",
    "        img = cv2.imread(imgPath, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.flatten()\n",
    "        img = img/255\n",
    "\n",
    "        label = imgPath.split(os.path.sep)[-2]\n",
    "        \n",
    "        lstData.append(img)\n",
    "        lstLabel.append(label)\n",
    "        \n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
    "            print(f\"[INFO] processed {i+1}/{len(image_paths)}\")\n",
    "            break\n",
    "\n",
    "    return lstData, lstLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10/42000\n",
      "Train x=(8, 784), y=(8, 1)\n",
      "Test x=(2, 784), y=(2, 1)\n"
     ]
    }
   ],
   "source": [
    "img_path = \"mnist/trainingSet/trainingSet\"\n",
    "# img_path = \"/content/mnist/trainingSet/trainingSet\"\n",
    "\n",
    "# Generate a list of all images\n",
    "image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "lstData, lstLabel = load_mnist_bypath(image_paths, verbose=10)\n",
    "data = np.array(lstData)\n",
    "labels = np.array(lstLabel)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "y_train = oneHotEncoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = oneHotEncoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(f\"Train x={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test x={x_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(100, activation=\"relu\", input_shape=shape),\n",
    "            layers.Dense(100, activation=\"relu\"),\n",
    "            layers.Dense(classes, activation=\"softmax\"),\n",
    "            ])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([3, 4], 'b'), ([1, 2], 'a')]\n"
     ]
    }
   ],
   "source": [
    "def create_clients_with_data_assignment(image_list, label_list, num_clients=10, initial=\"client\"):\n",
    "    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n",
    "                will be the data fragment - tuple of images and labels.\n",
    "        args:\n",
    "            image_list: a numpy array object with the images\n",
    "            label_list: list of binarized labels (one-hot encoded)\n",
    "            num_clients: number of customers (clients)\n",
    "            initial: the prefix of the clients, e.g., client_1\n",
    "     \"\"\"\n",
    "\n",
    "    # create list of client names\n",
    "    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n",
    "\n",
    "    # shuffle the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # shard the data and split it for each customer\n",
    "    size = len(data) // num_clients\n",
    "    shards = [data[i: i+size]  for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # Check if the fragment number is equal to the number of clients\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i]: shards[i]  for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, batch_size=32):\n",
    "    \"\"\" Receives a piece of data from a client and creates a tensorflow Dataset object in it\n",
    "        args:\n",
    "            data_shard: data and labels that make up a customer's data shard\n",
    "            b: batch size\n",
    "        return:\n",
    "            data tensorflow Dataset object\n",
    "    \"\"\"\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clients = 100\n",
    "per_clients_selected = 0.6\n",
    "batch_size = 32\n",
    "\n",
    "comms_round = 30\n",
    "lr = 0.01\n",
    "# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n",
    "# optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)\n",
    "optimizer = \"adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clients = create_clients_with_data_assignment(x_train, y_train, num_clients=num_clients, initial=\"client\")\n",
    "# Process and collate the training data for each client\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data, batch_size)\n",
    "\n",
    "# process and group test set\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "mlp_global = MLP()\n",
    "global_model = mlp_global.build(x_train.shape[1:], 10)\n",
    "\n",
    "\n",
    "# Global training loop collection\n",
    "for comm_round in range(comms_round):\n",
    "\n",
    "    # get the global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # randomize client data - using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    client_select = client_names[0: num_clients*per_clients_selected]\n",
    "\n",
    "#     # loop through each client and create a new local model\n",
    "#     for client in client_select:\n",
    "#         smlp_local = MLP()\n",
    "#         local_model = smlp_local.build(784, 10)\n",
    "#         local_model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "#         # set the weight of the local model to the weight of the global model\n",
    "#         local_model.set_weights(global_weights)\n",
    "\n",
    "#         # fit local model with client's data\n",
    "#         local_model.fit(clients_batched[client], epochs=1, verbose=0)\n",
    "\n",
    "#         # scale the model weights and add to list\n",
    "#         scaling_factor = weight_scalling_factor(clients_batched, client, client_select)\n",
    "#         scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "#         scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "#         # Check local accuracy\n",
    "#         # acc_l, loss_l = check_local_loss(client, local_model)\n",
    "\n",
    "#         # clear session to free memory after each communication round\n",
    "#         K.clear_session()\n",
    "\n",
    "#     # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "#     average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "#     # update global model\n",
    "#     global_model.set_weights(average_weights)\n",
    "\n",
    "#     # test global model and print out metrics after each communications round\n",
    "#     for (X_test, Y_test) in test_batched:\n",
    "#         global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
