{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://usaupload.com/72Eb/mnist.zip?download_token=122af6eb9e746e2774d3422cba7775dd9ca2a43aaa3156f4c0b5043723a24823\"\n",
    "# name = url.split(\"/\")[-1]\n",
    "# fileName = name.split(\"?\")[0]\n",
    "\n",
    "# !wget $url\n",
    "# !mv $name $fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !unzip mnist.zip;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "from imutils import paths\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras import models, layers\n",
    "\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Activation\n",
    "# from keras.layers import Dense\n",
    "# from keras import optimizers\n",
    "# from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySeed = 42\n",
    "np.random.seed(mySeed)\n",
    "random.seed(mySeed)\n",
    "tf.random.set_seed(mySeed)\n",
    "# torch.manual_seed(mySeed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_bypath(lst_image_paths, verbose=-1):\n",
    "    \"\"\" Expect to read images where each class is in a separate directory,\n",
    "        For example: images of type 0 are in folder 0\n",
    "    \"\"\"\n",
    "\n",
    "    lstData = list()\n",
    "    lstLabel = list()\n",
    "\n",
    "    for (i, imgPath) in enumerate(lst_image_paths):\n",
    "        img = cv2.imread(imgPath, cv2.IMREAD_GRAYSCALE)\n",
    "        img = img.flatten()\n",
    "        img = img/255\n",
    "        \n",
    "        label = imgPath.split(os.path.sep)[-2]\n",
    "        \n",
    "        lstData.append(img)\n",
    "        lstLabel.append(label)\n",
    "        \n",
    "        # show an update every `verbose` images\n",
    "        if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
    "            print(f\"[INFO] processed {i+1}/{len(lst_image_paths)}\")\n",
    "            break\n",
    "\n",
    "    return lstData, lstLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed 10000/42000\n",
      "Train x=(8000, 784), y=(8000, 3)\n",
      "Test x=(2000, 784), y=(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "img_path = \"mnist/trainingSet/trainingSet\"\n",
    "# img_path = \"/content/mnist/trainingSet/trainingSet\"\n",
    "\n",
    "# Generate a list of all images\n",
    "lst_image_paths = list(paths.list_images(img_path))\n",
    "\n",
    "lstData, lstLabel = load_mnist_bypath(lst_image_paths, verbose=10000)\n",
    "data = np.array(lstData)\n",
    "labels = np.array(lstLabel)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "oneHotEncoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "y_train = oneHotEncoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = oneHotEncoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(f\"Train x={x_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Test x={x_test.shape}, y={y_test.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_clients_with_data_assignment(image_list, label_list, num_clients=10, initial=\"client\"):\n",
    "    \"\"\" return: A dictionary with the customer id as the dictionary key and the value\n",
    "                will be the data fragment - tuple of images and labels.\n",
    "        args:\n",
    "            image_list: a numpy array object with the images\n",
    "            label_list: list of binarized labels (one-hot encoded)\n",
    "            num_clients: number of customers (clients)\n",
    "            initial: the prefix of the clients, e.g., client_1\n",
    "     \"\"\"\n",
    "\n",
    "    # create list of client names\n",
    "    client_names = [f\"{initial}_{i+1}\" for i in range(num_clients)]\n",
    "\n",
    "    # shuffle the data\n",
    "    data = list(zip(image_list, label_list))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    # shard the data and split it for each customer\n",
    "    size = len(data) // num_clients\n",
    "    shards = [data[i: i+size]  for i in range(0, size*num_clients, size)]\n",
    "\n",
    "    # Check if the fragment number is equal to the number of clients\n",
    "    assert(len(shards) == len(client_names))\n",
    "\n",
    "    return {client_names[i]: shards[i]  for i in range(len(client_names))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_data(data_shard, batch_size=32):\n",
    "    \"\"\" Receives a piece of data (imgs, labels) from a client and creates a tensorflow Dataset object in it\n",
    "        args:\n",
    "            data_shard: data and labels that make up a customer's data shard\n",
    "            batch_size: batch size\n",
    "        return:\n",
    "            data tensorflow Dataset object\n",
    "    \"\"\"\n",
    "    #seperate shard into data and labels lists\n",
    "    data, label = zip(*data_shard)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n",
    "    return dataset.shuffle(len(label)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weight, scalar):\n",
    "    \"\"\" Scale the model weights \"\"\"\n",
    "    \n",
    "    weight_final = []\n",
    "    for i in range(len(weight)):\n",
    "        weight_final.append(weight[i] * scalar)\n",
    "\n",
    "    return weight_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    @staticmethod\n",
    "    def build(shape, classes):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(100, activation=\"relu\", input_shape=shape),\n",
    "            layers.Dense(100, activation=\"relu\"),\n",
    "            layers.Dense(classes, activation=\"softmax\"),\n",
    "            ])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "num_clients = 2\n",
    "batch_size = 32\n",
    "\n",
    "client_select_rate = 1\n",
    "\n",
    "comms_round = 1\n",
    "client_epochs = 1\n",
    "lr = 0.01\n",
    "# optimizer = optimizers.Adam(learning_rate=lr, decay=lr/comms_round)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr, decay=lr/comms_round)\n",
    "# optimizer = \"adam\"\n",
    "loss = \"categorical_crossentropy\"\n",
    "metrics = [\"accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clients and batched data\n",
    "\n",
    "clients = create_clients_with_data_assignment(x_train, y_train, num_clients=num_clients, initial=\"client\")\n",
    "\n",
    "# Bached data with tensorflow data object\n",
    "clients_batched = dict()\n",
    "for (client_name, data) in clients.items():\n",
    "    clients_batched[client_name] = batch_data(data, batch_size)\n",
    "\n",
    "# Convert labels to tensorflow data object\n",
    "test_batched = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m scaled_local_weight_list\u001b[39m.\u001b[39mappend(scaled_weights)\n\u001b[0;32m     68\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mprint\u001b[39m(local_model\u001b[39m.\u001b[39;49mshape)\n\u001b[0;32m     70\u001b[0m \u001b[39mprint\u001b[39m(local_model\u001b[39m.\u001b[39mget_weights())\n\u001b[0;32m     72\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mlp_global = MLP()\n",
    "global_model = mlp_global.build(x_train.shape[1:], y_train.shape[-1])\n",
    "\n",
    "# Global training loop collection\n",
    "for comm_round in range(comms_round):\n",
    "\n",
    "    # global model's weights - will serve as the initial weights for all local models\n",
    "    global_weights = global_model.get_weights()\n",
    "\n",
    "    # initial list to collect local model weights after scalling\n",
    "    scaled_local_weight_list = list()\n",
    "\n",
    "    # randomize client - using keys\n",
    "    client_names = list(clients_batched.keys())\n",
    "    random.shuffle(client_names)\n",
    "    client_select = client_names[0: int(num_clients*client_select_rate)]\n",
    "\n",
    "    # calculate total training data across selected clients\n",
    "    # if all clients have a same length\n",
    "    global_count = len(clients[client_select[0]]) * len(client_select)\n",
    "\n",
    "    # loop through each client and create a new local model\n",
    "    for client in client_select:\n",
    "        mlp_local = MLP()\n",
    "        local_model = mlp_local.build(x_train.shape[1:], y_train.shape[-1])\n",
    "        local_model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "        # set the weight of the local model from the weight of the global model\n",
    "        local_model.set_weights(global_weights)\n",
    "\n",
    "        # fit local model with client's data\n",
    "        local_model.fit(clients_batched[client], epochs=client_epochs, verbose=0)\n",
    "\n",
    "        # scale the model weights and added to the list\n",
    "        local_count = len(clients[client])\n",
    "        scaling_factor = local_count / global_count\n",
    "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "        scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "        print(\"test\")\n",
    "        print(local_model.shape)\n",
    "        print(local_model.get_weights())\n",
    "        \n",
    "        break\n",
    "\n",
    "#         # Check local accuracy\n",
    "#         # acc_l, loss_l = check_local_loss(client, local_model)\n",
    "\n",
    "#         # clear session to free memory after each communication round\n",
    "#         K.clear_session()\n",
    "\n",
    "#     # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "#     average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "#     # update global model\n",
    "#     global_model.set_weights(average_weights)\n",
    "\n",
    "#     # test global model and print out metrics after each communications round\n",
    "#     for (X_test, Y_test) in test_batched:\n",
    "#         global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
