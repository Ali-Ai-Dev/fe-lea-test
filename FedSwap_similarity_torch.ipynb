{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import cca_core, cka_core_torch, dcka\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import argparse\n",
    "import shutil\n",
    "import h5py\n",
    "from FEMNIST_by_write import get_client_datasets, get_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducing code\n",
    "mySeed = 42\n",
    "random.seed(mySeed)  # Python random module.\n",
    "np.random.seed(mySeed)  # Numpy module.\n",
    "torch.manual_seed(mySeed)\n",
    "torch.cuda.manual_seed(mySeed)\n",
    "torch.cuda.manual_seed_all(mySeed)  # if you are using multi-GPU.\n",
    "\n",
    "dataloader_generator = torch.Generator()\n",
    "dataloader_generator.manual_seed(mySeed)\n",
    "\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_range(mini, maxi):\n",
    "    \"\"\"Return function handle of an argument type function for \n",
    "       ArgumentParser checking a float range: mini <= arg <= maxi\n",
    "         mini - minimum acceptable argument\n",
    "         maxi - maximum acceptable argument\"\"\"\n",
    "\n",
    "    # Define the function with default arguments\n",
    "    def float_range_checker(arg):\n",
    "        \"\"\"New Type function for argparse - a float within predefined range.\"\"\"\n",
    "\n",
    "        try:\n",
    "            f = float(arg)\n",
    "        except ValueError:    \n",
    "            raise argparse.ArgumentTypeError(\"must be a floating point number\")\n",
    "        if f < mini or f > maxi:\n",
    "            raise argparse.ArgumentTypeError(\"must be in range [\" + str(mini) + \" .. \" + str(maxi)+\"]\")\n",
    "        return f\n",
    "\n",
    "    # Return function handle to checking function\n",
    "    return float_range_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters (Auto)\n",
    "parser = argparse.ArgumentParser(prog='FedSwap_Similarity',\n",
    "                                 description=\"FedSwap_Similarity: Converging Federated Learning Faster\",\n",
    "                                 epilog=\"Written by Ali Bozorgzad\")\n",
    "\n",
    "parser.add_argument(\"--dataset\", \"-d\", dest=\"dataset_name\", type=str, default=\"CIFAR10\",\n",
    "                    help=\"dataset name\", choices=['MNIST', 'CIFAR10', 'CINIC10', 'FEMNIST', 'FEMNISTwriter'])\n",
    "parser.add_argument(\"--NN_type\", \"-n\", dest=\"neural_network_type\", type=str, default=\"Conv2\",\n",
    "                    help=\"neural network type\", choices=['MLP1', 'MLP2','Conv1','Conv2','Conv3', 'Conv4', 'Conv5'])\n",
    "parser.add_argument(\"--similarity_mode\", \"-m\", dest=\"similarity_mode\", type=str, default=\"cka_rbf\",\n",
    "                    help=\"similarity mode\", choices=['cca', 'cka_linear', 'cka_rbf', 'sum_diff', 'dcka'])\n",
    "parser.add_argument(\"--swap_mode\", \"-o\", dest=\"swap_mode\", type=str, default=\"best\",\n",
    "                    help=\"swap mode\", choices=['greedy', 'best'])\n",
    "parser.add_argument(\"--num_clients\", \"-c\", dest=\"num_clients\", type=int, default=\"10\",\n",
    "                    help=\"number of clients, except for 'FEMNISTwriter', cause it fixed\")\n",
    "parser.add_argument(\"--batch_size\", \"-b\", dest=\"batch_size\", type=int, default=\"64\",\n",
    "                    help=\"batch size\")\n",
    "parser.add_argument(\"--total_steps\", \"-t\", dest=\"total_steps\", type=int, default=\"301\",\n",
    "                    help=\"total steps\")\n",
    "parser.add_argument(\"--client_per\", \"-p\", dest=\"client_select_percentage\", type=float_range(1e-2, 1), default=\"1\",\n",
    "                    help=\"client selection percentage, between [1e-2...1] 1 is 100%%\")\n",
    "parser.add_argument(\"--swap_per\", \"-w\", dest=\"swap_percentage\", type=float_range(1e-5, 1), default=\"1\",\n",
    "                    help=\"swap percentage, between [1e-5...1] 1 is 100%%\")\n",
    "parser.add_argument(\"--clients_data_distribution\", \"-u\", dest=\"clients_data_distribution\", type=str, default=\"equal\",\n",
    "                    help=\"how distribute train data between clients\", choices=['equal', 'random', 'normal'])\n",
    "parser.add_argument(\"--random_split\", \"-r\", dest=\"data_random_split\", type=int, default=\"1\",\n",
    "                    help=\"data random split between clients\", choices=[0, 1])\n",
    "parser.add_argument(\"--learning_rate\", \"-l\", dest=\"learning_rate\", type=float, default=\"1e-3\",\n",
    "                    help=\"learning rate\")\n",
    "parser.add_argument(\"--client_epochs\", \"-e\", dest=\"client_epochs\", type=int, default=\"1\",\n",
    "                    help=\"client epochs\")\n",
    "parser.add_argument(\"--print_eval_each_step\", \"-i\", dest=\"print_eval_each_step\", type=int, default=\"1\",\n",
    "                    help=\"set 0 means, only print when assign global weights to each client\", choices=[0, 1])\n",
    "parser.add_argument(\"--swap_step\", \"-s\", dest=\"swap_step\", type=int, default=\"3\",\n",
    "                    help=\"swap clients weights after X step\")\n",
    "parser.add_argument(\"--num_swap_bet_avg\", \"-a\", dest=\"n_swap_bet_avg_p1\", type=int, default=\"10\",\n",
    "                    help=\"number of swap between avg, plus one to your number= if need 2 swap between avg, enter 3\")\n",
    "parser.add_argument(\"--remain\", dest=\"remain\", type=float_range(1e-3, 1), default=\"1\",\n",
    "                    help=\"remain %% of dataset for running faster in test, between [1e-3...1] 1 is 100%%, except for 'FEMNISTwriter'\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "\n",
    "\n",
    "dataset_name = args.dataset_name\n",
    "neural_network_type = args.neural_network_type\n",
    "similarity_mode = args.similarity_mode\n",
    "swap_mode = args.swap_mode\n",
    "\n",
    "num_clients = args.num_clients\n",
    "batch_size = args.batch_size\n",
    "total_steps = args.total_steps\n",
    "client_select_percentage = args.client_select_percentage\n",
    "swap_percentage = args.swap_percentage\n",
    "clients_data_distribution = args.clients_data_distribution\n",
    "data_random_split = args.data_random_split\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = args.client_epochs\n",
    "print_eval_each_step = args.print_eval_each_step\n",
    "\n",
    "swap_step = args.swap_step\n",
    "n_swap_bet_avg_p1 = args.n_swap_bet_avg_p1\n",
    "\n",
    "remain = args.remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters (Manual)\n",
    "dataset_name = \"MNIST\" # 'MNIST' or 'CIFAR10' or 'CINIC10' or 'FEMNIST' or 'FEMNISTwriter'\n",
    "neural_network_type = \"Conv4\" # 'MLP1' or 'MLP2' or'Conv1' or'Conv2' or'Conv3' or'Conv4' or'Conv5'\n",
    "similarity_mode = \"cka_rbf\" # 'cca' or 'cka_linear' or 'cka_rbf' or 'sum_diff' or 'dcka'\n",
    "swap_mode = \"best\" # 'greedy' or 'best'\n",
    "\n",
    "num_clients = 10 # except for 'FEMNISTwriter'\n",
    "batch_size = 16\n",
    "total_steps = 64\n",
    "client_select_percentage = 1.0\n",
    "swap_percentage = 1\n",
    "clients_data_distribution = \"normal\" # 'equal' or 'random' or 'normal'\n",
    "data_random_split = 1 # 0 or 1\n",
    "\n",
    "learning_rate = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "client_epochs = 1\n",
    "print_eval_each_step = 1 # 0 or 1 - if set 0 means, only print when assign global weights to each client\n",
    "\n",
    "swap_step = 3\n",
    "n_swap_bet_avg_p1 = 10 # p1=plus one to your number, if need 2 swap between avg, enter 3\n",
    "\n",
    "remain = 0.001 # Remove some data for running faster in test, except for 'FEMNISTwriter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_log_file_name: 'FSS_MNIST_Conv4_cka_rbf_best_10c_16b_1.0cp_1sp_normal_1rs_0.0001lr_1ce_1pes_3_10_step'\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "client_selects = None\n",
    "client_weights = None\n",
    "\n",
    "passed_steps = 0\n",
    "is_print_eval = False\n",
    "\n",
    "start_bold = \"\\u001b[1m\"\n",
    "end_bold = \"\\033[0m\"\n",
    "os.system(\"color\")\n",
    "color = {\n",
    "    \"ENDC\": end_bold,\n",
    "    \"Bold\": start_bold,\n",
    "}\n",
    "\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "os.makedirs(\"save_log\", exist_ok=True)\n",
    "\n",
    "neural_network_classname = f\"Neural_Network_{neural_network_type}\"\n",
    "\n",
    "if dataset_name == 'FEMNISTwriter':\n",
    "    num_clients = 3597 # or 'num_clients = len(writers)' but put fstring after 'writers' is fill\n",
    "    \n",
    "save_file_name_pre = f\"\"\"FSS\n",
    "_{dataset_name}_{neural_network_type}_{similarity_mode}_{swap_mode}\n",
    "_{num_clients}c_{batch_size}b_{client_select_percentage}cp_{swap_percentage}sp\n",
    "_{clients_data_distribution}_{data_random_split}rs_{learning_rate}lr_{client_epochs}ce\n",
    "_{print_eval_each_step}pes_{swap_step}_{n_swap_bet_avg_p1}_step\"\"\"\n",
    "save_file_name_pre = save_file_name_pre.replace(\"\\n\", \"\")\n",
    "print(f\"save_log_file_name: '{save_file_name_pre}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 'cuda' device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using '{device}' device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_femnist_dataset_ready_to_read():\n",
    "    ### ToDo: Download and extract dataset to datasets dir\n",
    "    cur_dir = os.getcwd()\n",
    "    datasets_dir = os.path.join(cur_dir, \"datasets\")\n",
    "\n",
    "    if os.path.isdir(os.path.join(datasets_dir, \"by_class\")):\n",
    "        dataset_dir = os.path.join(datasets_dir, \"FEMNIST\")\n",
    "        os.rename(os.path.join(datasets_dir, \"by_class\"), dataset_dir)\n",
    "\n",
    "        for i, class_dir in enumerate(os.listdir(dataset_dir)):\n",
    "            class_imgs = os.path.join(dataset_dir, class_dir, \"train_\"+class_dir)\n",
    "            shutil.move(class_imgs, dataset_dir)\n",
    "            shutil.rmtree(os.path.join(dataset_dir, class_dir))\n",
    "            print(f\"Ready to be read and preprocess, class {i}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "torch.Size([1, 28, 28])\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "if dataset_name == 'FEMNIST':\n",
    "    download_femnist_dataset_ready_to_read()\n",
    "    \n",
    "    full_data = datasets.ImageFolder(\n",
    "        'datasets/FEMNIST',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize(size=(28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(62).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "    lst_range = np.arange(0, len(full_data))\n",
    "    lst_random = np.random.permutation(lst_range)\n",
    "    test_indices = lst_random[: int(len(lst_random)*0.1)]\n",
    "    train_indices = list(filter(lambda i: i not in test_indices, lst_range))\n",
    "\n",
    "    train_data = torch.utils.data.Subset(full_data, train_indices)\n",
    "    test_data = torch.utils.data.Subset(full_data, test_indices)\n",
    "\n",
    "elif dataset_name == 'FEMNISTwriter':\n",
    "    dataset_dir = \"datasets\\FEMNIST_by_write\\write_all_labels.hdf5\"\n",
    "    binary_data_file = h5py.File(dataset_dir, \"r\")\n",
    "\n",
    "    writers = sorted(binary_data_file.keys())\n",
    "    dic_train_indices = dict()\n",
    "    dic_test_indices = dict()\n",
    "    len_train_data = 0\n",
    "\n",
    "    for writer in writers:\n",
    "        labels = binary_data_file[writer]['labels']\n",
    "\n",
    "        lst_range = np.arange(0, len(labels))\n",
    "        lst_random = np.random.permutation(lst_range)\n",
    "        test_indices = lst_random[: int(len(lst_random)*0.1)]\n",
    "        train_indices = list(filter(lambda i: i not in test_indices, lst_range))\n",
    "        len_train_data += len(train_indices)\n",
    "\n",
    "        dic_train_indices[writer] = train_indices\n",
    "        dic_test_indices[writer] = test_indices\n",
    "\n",
    "elif dataset_name == 'CINIC10':\n",
    "    dataset_dir = 'datasets/CINIC-10'\n",
    "    cinic_mean = [0.47889522, 0.47227842, 0.43047404]\n",
    "    cinic_std = [0.24205776, 0.23828046, 0.25874835]\n",
    "    \n",
    "    train_data = datasets.ImageFolder(\n",
    "        dataset_dir + '/train_valid',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "    test_data = datasets.ImageFolder(\n",
    "        dataset_dir + '/test',\n",
    "        transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=cinic_mean, std=cinic_std),\n",
    "        ]),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "else: # 'MNIST' or 'CIFAR10'\n",
    "    running_dataset = getattr(datasets, dataset_name)\n",
    "\n",
    "    train_data = running_dataset(\n",
    "        root=\"datasets\",\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),    \n",
    "    )\n",
    "\n",
    "    test_data = running_dataset(\n",
    "        root=\"datasets\",\n",
    "        train=False,\n",
    "        download=True,\n",
    "        transform=ToTensor(),\n",
    "        target_transform=Lambda(lambda y: torch.zeros(10).scatter_(dim=0, index=torch.tensor(y), value=1)),\n",
    "    )\n",
    "\n",
    "if dataset_name != 'FEMNISTwriter':\n",
    "    print(len(train_data))\n",
    "    assert len(train_data) > (10*num_clients), (\"we expect each client have some data\")\n",
    "    print(train_data[0][0].shape)\n",
    "    print(train_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remain data parameter: 0.001\n",
      "full train_data size: 60000\n",
      "cutted train_data size: 60\n",
      "full test_data size: 10000\n",
      "cutted test_data size: 10\n"
     ]
    }
   ],
   "source": [
    "if dataset_name != 'FEMNISTwriter':\n",
    "    # Remove some data for running faster in test\n",
    "    print(\"remain data parameter:\", remain)\n",
    "    print(\"full train_data size:\", len(train_data))\n",
    "    train_data = torch.utils.data.Subset(train_data, range(0, int(len(train_data)*remain)))\n",
    "    print(\"cutted train_data size:\", len(train_data))\n",
    "\n",
    "    print(\"full test_data size:\", len(test_data))\n",
    "    test_data = torch.utils.data.Subset(test_data, range(0, int(len(test_data)*remain)))\n",
    "    print(\"cutted test_data size:\", len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_one_normalized_random_numbers(size, mean=0, std=1):\n",
    "    numbers = np.random.normal(loc=mean, scale=std, size=size)\n",
    "    numbers = np.abs(numbers) # Ensure all numbers are positive\n",
    "    numbers /= np.sum(numbers) # Normalize numbers to make their sum=1\n",
    "    \n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_thresh_for_sum_one_random_numbers(sum_one_random_numbers):\n",
    "    # set a min_thresh: ref='https://stackoverflow.com/a/62911965/4464934'\n",
    "    min_thresh = 1 / len(train_data)\n",
    "    rand_prop = 1 - num_clients * min_thresh\n",
    "    random_numbers_min_thresh = (sum_one_random_numbers * rand_prop) + min_thresh\n",
    "\n",
    "    return random_numbers_min_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_data_size_for_sum_one_random_numbers(sum_one_random_numbers):\n",
    "    random_numbers_min_thresh = min_thresh_for_sum_one_random_numbers(sum_one_random_numbers)\n",
    "    client_data_size = np.floor(random_numbers_min_thresh*len(train_data)).astype(int)\n",
    "\n",
    "    return client_data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data to clients\n",
    "if dataset_name != 'FEMNISTwriter':\n",
    "    if clients_data_distribution == \"equal\":\n",
    "        client_data_size = np.array([len(train_data)//num_clients]*num_clients)\n",
    "\n",
    "    elif clients_data_distribution == \"random\":\n",
    "        # random numbers with sum=1\n",
    "        sum_one_random_numbers = np.random.dirichlet(np.ones(num_clients))\n",
    "        client_data_size = client_data_size_for_sum_one_random_numbers(sum_one_random_numbers)\n",
    "        \n",
    "    elif clients_data_distribution == \"normal\":\n",
    "        sum_one_random_numbers = sum_one_normalized_random_numbers(num_clients)\n",
    "        client_data_size = client_data_size_for_sum_one_random_numbers(sum_one_random_numbers)\n",
    "\n",
    "    data_remain = len(train_data) - sum(client_data_size)\n",
    "    for i in range(data_remain):\n",
    "        client_data_size[-1-i] += 1\n",
    "\n",
    "    if data_random_split:\n",
    "        client_datasets = torch.utils.data.random_split(train_data, client_data_size)\n",
    "    else:\n",
    "        client_datasets = list()\n",
    "        i = 0\n",
    "        for j in client_data_size:\n",
    "            client_datasets.append(torch.utils.data.Subset(train_data, range(i, i+j)))\n",
    "            i += j\n",
    "else:\n",
    "    client_datasets = get_client_datasets(writers, binary_data_file, dic_train_indices)\n",
    "    test_data = get_test_dataset(writers, binary_data_file, dic_test_indices)\n",
    "\n",
    "    print(f\"num_clients in FEMNISTwriter: {len(client_datasets)}\")\n",
    "    print(len_train_data)\n",
    "    print(client_datasets[0][0][0].shape)\n",
    "    print(client_datasets[0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloader for each client\n",
    "client_dataloaders = np.zeros(num_clients, dtype=object)\n",
    "for i, dataset in enumerate(client_datasets):\n",
    "    client_dataloaders[i] = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                                       shuffle=True, generator=dataloader_generator,)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,\n",
    "                             shuffle=True, generator=dataloader_generator,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_out_conv_max_layers(in_w, in_h, kernels, strides, paddings=None, dilations=None):\n",
    "    # In MaxPool2d, strides must same with kernels\n",
    "    if paddings == None:\n",
    "        paddings = np.zeros(len(kernels))\n",
    "    \n",
    "    if dilations == None:\n",
    "        dilations = np.ones(len(kernels))\n",
    "    \n",
    "    out_w = in_w\n",
    "    out_h = in_h\n",
    "    for ker, pad, dil, stri in zip(kernels, paddings, dilations, strides):\n",
    "        out_w = np.floor((out_w + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "        out_h = np.floor((out_h + 2*pad - dil * (ker-1) - 1)/stri + 1)\n",
    "\n",
    "    return int(out_w), int(out_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define MLP models\n",
    "input_flat_size = torch.flatten(test_data[0][0]).shape[0]\n",
    "nClasses = test_data[0][1].shape[0]\n",
    "\n",
    "class Neural_Network_MLP1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 100)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(100, 99)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(99, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "class Neural_Network_MLP2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(OrderedDict([\n",
    "            ('fc1', nn.Linear(input_flat_size, 256)),\n",
    "            ('relu1', nn.ReLU()),\n",
    "            ('fc2', nn.Linear(256, 128)),\n",
    "            ('relu2', nn.ReLU()),\n",
    "            ('fc3', nn.Linear(128, 64)),\n",
    "            ('relu3', nn.ReLU()),\n",
    "            ('fc4', nn.Linear(64, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Convolutional models\n",
    "input_channels, input_width, input_height = test_data[0][0].shape\n",
    "\n",
    "conv_kernel1 = 3\n",
    "max_kernel1 = 2\n",
    "kernels = [conv_kernel1, max_kernel1, conv_kernel1]\n",
    "strides = [1, max_kernel1, 1]\n",
    "out_w1, out_h1 = calc_out_conv_max_layers(input_width, input_height, kernels, strides)\n",
    "\n",
    "class Neural_Network_Conv1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel1, stride=1, padding=0)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel1)),\n",
    "            ('conv2', nn.Conv2d(32, 64, kernel_size=conv_kernel1)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w1*out_h1, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "conv_kernel2 = 3\n",
    "max_kernel2 = 2\n",
    "kernels = [conv_kernel2, max_kernel2, conv_kernel2, max_kernel2]\n",
    "strides = [1, max_kernel2, 1, max_kernel2]\n",
    "out_w2, out_h2 = calc_out_conv_max_layers(input_width, input_height, kernels, strides)\n",
    "\n",
    "class Neural_Network_Conv2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel2, stride=1, padding=0)),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel2)),\n",
    "            ('conv2', nn.Conv2d(32, 64, kernel_size=conv_kernel2)),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel2)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w2*out_h2, 100)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('fc2', nn.Linear(100, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "conv_kernel3 = 5\n",
    "max_kernel3 = 2\n",
    "kernels = [conv_kernel3, max_kernel3, conv_kernel3, max_kernel3]\n",
    "strides = [1, max_kernel3, 1, max_kernel3]\n",
    "paddings = [1, 1, 1, 1]\n",
    "out_w3, out_h3 = calc_out_conv_max_layers(input_width, input_height, kernels, strides, paddings)\n",
    "\n",
    "class Neural_Network_Conv3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel3, stride=1, padding='same')),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel3)),\n",
    "            ('conv2', nn.Conv2d(32, 64, kernel_size=conv_kernel3, padding='same')),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel3)),\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w3*out_h3, 2048)),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('fc2', nn.Linear(2048, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "\n",
    "# A Good model for MNIST dataset, Acc=99.6%\n",
    "# https://medium.com/@BrendanArtley/mnist-keras-simple-cnn-99-6-731b624aee7f\n",
    "# Layer order: Activation -> Normalization -> Pooling -> Dropout\n",
    "conv_kernel4 = 3\n",
    "max_kernel4 = 2\n",
    "kernels = [conv_kernel4, conv_kernel4, max_kernel4, conv_kernel4, conv_kernel4, max_kernel4]\n",
    "strides = [1, 1, max_kernel4, 1, 1, max_kernel4]\n",
    "paddings = [1, 1, 0, 1, 1, 0]\n",
    "out_w4, out_h4 = calc_out_conv_max_layers(input_width, input_height, kernels, strides, paddings)\n",
    "\n",
    "class Neural_Network_Conv4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 32, kernel_size=conv_kernel4, stride=1, padding='same')),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('bn1', nn.BatchNorm2d(32)),\n",
    "            ('conv2', nn.Conv2d(32, 32, kernel_size=conv_kernel4, padding='same')),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('bn2', nn.BatchNorm2d(32)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel4)),\n",
    "            ('drop1', nn.Dropout(p=0.25)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(32, 64, kernel_size=conv_kernel4, stride=1, padding='same')),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('bn3', nn.BatchNorm2d(64)),\n",
    "            ('conv4', nn.Conv2d(64, 64, kernel_size=conv_kernel4, padding='same')),\n",
    "            ('relu4', nn.ReLU(inplace=True)),\n",
    "            ('bn4', nn.BatchNorm2d(64)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel4)),\n",
    "            ('drop2', nn.Dropout(p=0.25)),\n",
    "\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(64*out_w4*out_h4, 512)),\n",
    "            ('relu5', nn.ReLU(inplace=True)),\n",
    "            ('bn5', nn.BatchNorm1d(512)),\n",
    "            ('drop3', nn.Dropout(p=0.25)),\n",
    "\n",
    "            ('fc2', nn.Linear(512, 1024)),\n",
    "            ('relu6', nn.ReLU(inplace=True)),\n",
    "            ('bn6', nn.BatchNorm1d(1024)),\n",
    "            ('drop4', nn.Dropout(p=0.5)),\n",
    "\n",
    "            ('fc3', nn.Linear(1024, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data\n",
    "\n",
    "\n",
    "\n",
    "# Model for CIFAR10 dataset, FedSwap paper\n",
    "conv_kernel5 = 3\n",
    "max_kernel5 = 2\n",
    "kernels = [conv_kernel5, max_kernel5, conv_kernel5, max_kernel5, conv_kernel5, max_kernel5]\n",
    "strides = [1, max_kernel5, 1, max_kernel5, 1, max_kernel5]\n",
    "paddings = [1, 0, 1, 0, 1, 0]\n",
    "out_w5, out_h5 = calc_out_conv_max_layers(input_width, input_height, kernels, strides, paddings)\n",
    "\n",
    "class Neural_Network_Conv5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features_stack = nn.Sequential(OrderedDict([\n",
    "            ('conv1', nn.Conv2d(input_channels, 64, kernel_size=conv_kernel5, stride=1, padding='same')),\n",
    "            ('relu1', nn.ReLU(inplace=True)),\n",
    "            ('pool1', nn.MaxPool2d(kernel_size=max_kernel5)),\n",
    "\n",
    "            ('conv2', nn.Conv2d(64, 128, kernel_size=conv_kernel5, padding='same')),\n",
    "            ('relu2', nn.ReLU(inplace=True)),\n",
    "            ('pool2', nn.MaxPool2d(kernel_size=max_kernel5)),\n",
    "\n",
    "            ('conv3', nn.Conv2d(128, 256, kernel_size=conv_kernel5, padding='same')),\n",
    "            ('relu3', nn.ReLU(inplace=True)),\n",
    "            ('pool3', nn.MaxPool2d(kernel_size=max_kernel5)),\n",
    "\n",
    "            ('flat', nn.Flatten()),\n",
    "            ('fc1', nn.Linear(256*out_w5*out_h5, 1024)),\n",
    "            ('relu4', nn.ReLU(inplace=True)),\n",
    "            ('fc2', nn.Linear(1024, nClasses)),\n",
    "        ]))\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.features_stack(x)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "    def get_weights(self):\n",
    "        return list(self.parameters())\n",
    "    \n",
    "    def set_weights(self, parameters_list):\n",
    "        for i, param in enumerate(self.parameters()):\n",
    "            param.data = parameters_list[i].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_clients_and_assign_weights(global_weights):\n",
    "    global client_selects\n",
    "    global client_weights\n",
    "\n",
    "    lst = np.arange(0, num_clients)\n",
    "    np.random.shuffle(lst)\n",
    "    client_selects = lst[: int(len(lst)*client_select_percentage)]\n",
    "\n",
    "    client_weights = {i: copy.deepcopy(global_weights)  for i in client_selects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Network_Conv4(\n",
      "  (features_stack): Sequential(\n",
      "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu1): ReLU(inplace=True)\n",
      "    (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu2): ReLU(inplace=True)\n",
      "    (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (drop1): Dropout(p=0.25, inplace=False)\n",
      "    (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu3): ReLU(inplace=True)\n",
      "    (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (relu4): ReLU(inplace=True)\n",
      "    (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (drop2): Dropout(p=0.25, inplace=False)\n",
      "    (flat): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=3136, out_features=512, bias=True)\n",
      "    (relu5): ReLU(inplace=True)\n",
      "    (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop3): Dropout(p=0.25, inplace=False)\n",
      "    (fc2): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    (relu6): ReLU(inplace=True)\n",
      "    (bn6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (drop4): Dropout(p=0.5, inplace=False)\n",
      "    (fc3): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instantiate of a class with string value!\n",
    "global_model = globals()[neural_network_classname]().to(device)\n",
    "global_weights = global_model.get_weights()\n",
    "select_clients_and_assign_weights(global_weights)\n",
    "print(global_model)\n",
    "\n",
    "global_history = {\"times\": {\"train\":list(), \"swap\":list(), \"step\":list()},\n",
    "                  \"accuracy\": list(),\n",
    "                  \"loss\": list()}\n",
    "\n",
    "# Load saved state & log\n",
    "last_saved = sorted(glob.glob(f\"save_log/{save_file_name_pre}_*.npz\"), key=os.path.getmtime)\n",
    "if last_saved:\n",
    "    last_saved = last_saved[-1]\n",
    "    passed_steps = int(last_saved.split(\"_\")[-1].split(\".\")[0]) + 1\n",
    "\n",
    "    npzFile = np.load(last_saved, allow_pickle=True)\n",
    "    client_selects = npzFile[\"client_selects\"]\n",
    "    client_weights = npzFile[\"client_weights\"].item()\n",
    "    global_history = npzFile[\"global_history\"].item()\n",
    "    dataloader_generator_state = torch.tensor(npzFile[\"dataloader_generator_state\"])\n",
    "    random_state = tuple(npzFile[\"random_state_ndarray\"])\n",
    "    np_random_state = tuple(npzFile[\"np_random_state_ndarray\"])\n",
    "    # torch_rng_states = npzFile[\"torch_rng_states_ndarray\"]\n",
    "    npzFile.close()\n",
    "\n",
    "    dataloader_generator.set_state(dataloader_generator_state)\n",
    "    random.setstate(random_state)\n",
    "    np.random.set_state(np_random_state)\n",
    "    # torch.set_rng_state(torch_rng_states[0])\n",
    "    # torch.cuda.set_rng_state(torch_rng_states[1])\n",
    "    # torch.cuda.set_rng_state_all(torch_rng_states[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_model_weights(weights, scalar):\n",
    "    \"\"\" Scale the model weights \"\"\"\n",
    "\n",
    "    scaled_weights = list()\n",
    "    for i in range(len(weights)):\n",
    "        scaled_weights.append(weights[i] * scalar)\n",
    "\n",
    "    return scaled_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_scaled_weights(client_scaled_weights):\n",
    "    \"\"\" Return the sum of the listed scaled weights.\n",
    "        axis_O is equivalent to the average weight of the weights \"\"\"\n",
    "\n",
    "    avg_weights = list()\n",
    "    # get the average gradient accross all client gradients\n",
    "    for gradient_list_tuple in zip(*client_scaled_weights):\n",
    "        gradient_list_tuple = [tensors.tolist()  for tensors in gradient_list_tuple]\n",
    "        layer_mean = torch.sum(torch.tensor(gradient_list_tuple), axis=0).to(device)\n",
    "        avg_weights.append(layer_mean)\n",
    "\n",
    "    return avg_weights\n",
    "\n",
    "\n",
    "### Explaining the function with example ###\n",
    "# t = (torch.tensor([[[2, 3],[3, 4]], [[3, 4],[4, 5]], [[4, 5],[5, 6]]]),\n",
    "#      torch.tensor([[[5, 6],[6, 7]], [[6, 7],[7, 8]], [[7, 8],[8, 9]]]))\n",
    "# t = [i.tolist() for i in t]\n",
    "# for y in zip(*t):\n",
    "#     print(y)\n",
    "#     print(torch.sum(torch.tensor(y), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_avg():\n",
    "    # calculate total training data across clients\n",
    "    global_count = 0\n",
    "    for client in client_selects:\n",
    "        global_count += len(client_dataloaders[client].dataset)\n",
    "\n",
    "    # initial list to collect clients weight after scalling\n",
    "    client_scaled_weights = list()\n",
    "    for client in client_selects:\n",
    "        local_count = len(client_dataloaders[client].dataset)\n",
    "        scaling_factor = local_count / global_count\n",
    "        scaled_weights = scale_model_weights(client_weights[client], scaling_factor)\n",
    "        client_scaled_weights.append(scaled_weights)\n",
    "\n",
    "    # to get the average over all the clients model, we simply take the sum of the scaled weights\n",
    "    avg_weights = sum_scaled_weights(client_scaled_weights)\n",
    "\n",
    "    return avg_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fed_swap(client, swap_client):\n",
    "    temp_weight = client_weights[swap_client]\n",
    "    client_weights[swap_client] = client_weights[client]\n",
    "    client_weights[client] = temp_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_similarity(weights_1, weights_2):\n",
    "    lst_layer_similarity = list()\n",
    "    for i in range(len(weights_1)):\n",
    "        len_shape = len(weights_1[i].shape)\n",
    "\n",
    "        if len_shape == 1: # bias weights\n",
    "            continue\n",
    "\n",
    "        # change it to two dimention\n",
    "        if similarity_mode == \"cca\": # only CCA run in cpu\n",
    "            weights_1_two_dim = weights_1[i].reshape(weights_1[i].shape[0], -1).cpu().detach().numpy()\n",
    "            weights_2_two_dim = weights_2[i].reshape(weights_2[i].shape[0], -1).cpu().detach().numpy()\n",
    "\n",
    "            if weights_1_two_dim.shape[0] > weights_1_two_dim.shape[1]:\n",
    "                weights_1_two_dim = weights_1_two_dim.T\n",
    "                weights_2_two_dim = weights_2_two_dim.T\n",
    "\n",
    "        else:\n",
    "            weights_1_two_dim = weights_1[i].reshape(weights_1[i].shape[0], -1)\n",
    "            weights_2_two_dim = weights_2[i].reshape(weights_2[i].shape[0], -1)\n",
    "\n",
    "            # if cuda have memory to compute (1024 is a hyperparameter limit)\n",
    "            if weights_1_two_dim.shape[1] > weights_1_two_dim.shape[0] and weights_1_two_dim.shape[1] <= 1024:\n",
    "                weights_1_two_dim = weights_1_two_dim.T\n",
    "                weights_2_two_dim = weights_2_two_dim.T\n",
    "            \n",
    "            if weights_1_two_dim.shape[0] > 1024 and weights_1_two_dim.shape[1] < 1024:\n",
    "                weights_1_two_dim = weights_1_two_dim.T\n",
    "                weights_2_two_dim = weights_2_two_dim.T\n",
    "\n",
    "        if similarity_mode == \"dcka\":\n",
    "            Xn = dcka.normalize(weights_1_two_dim)\n",
    "            Yn = dcka.normalize(weights_2_two_dim)\n",
    "\n",
    "            L_X = torch.matmul(Xn, Xn.T)\n",
    "            L_Y = torch.matmul(Yn, Yn.T)\n",
    "\n",
    "            layer_similarity, _, _ = dcka.linear_CKA(\n",
    "                L_X=L_X,\n",
    "                L_Y=L_Y,\n",
    "                input_confounders=weights_1_two_dim,\n",
    "                device=device,\n",
    "            )\n",
    "        \n",
    "        elif similarity_mode == \"sum_diff\":\n",
    "            layer_similarity = -torch.sum(torch.abs(weights_1_two_dim - weights_2_two_dim))\n",
    "\n",
    "        elif similarity_mode == \"cka_rbf\":\n",
    "            gram_rbf_1 = cka_core_torch.gram_rbf(weights_1_two_dim, 0.5)\n",
    "            gram_rbf_2 = cka_core_torch.gram_rbf(weights_2_two_dim, 0.5)\n",
    "            layer_similarity = cka_core_torch.cka(gram_rbf_1, gram_rbf_2)\n",
    "            \n",
    "        elif similarity_mode == \"cka_linear\":\n",
    "            gram_linear_1 = cka_core_torch.gram_linear(weights_1_two_dim)\n",
    "            gram_linear_2 = cka_core_torch.gram_linear(weights_2_two_dim)\n",
    "            layer_similarity = cka_core_torch.cka(gram_linear_1, gram_linear_2)\n",
    "\n",
    "        elif similarity_mode == \"cca\":\n",
    "            # Mean Squared CCA similarity\n",
    "            results = cca_core.get_cca_similarity(weights_1_two_dim, weights_2_two_dim, epsilon=1e-10, verbose=False)\n",
    "            layer_similarity = np.mean(results[\"cca_coef1\"])**2\n",
    "\n",
    "        lst_layer_similarity.append(layer_similarity.item()) \n",
    "\n",
    "    return np.mean(lst_layer_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_swapping():\n",
    "    lst_remain_swap = client_selects.copy()\n",
    "    for _ in range(int((len(client_selects)//2)*swap_percentage)):\n",
    "        random_index = random.randint(0, len(lst_remain_swap)-1)\n",
    "        swap_client_base = lst_remain_swap[random_index]\n",
    "        lst_remain_swap = np.delete(lst_remain_swap, random_index)\n",
    "\n",
    "        lst_similarity = list()\n",
    "        for remain_client in lst_remain_swap:\n",
    "            similarity = model_similarity(client_weights[swap_client_base], client_weights[remain_client])\n",
    "            lst_similarity.append(similarity)\n",
    "\n",
    "        min_similarity_index = np.argmin(lst_similarity)\n",
    "        swap_client_dest = lst_remain_swap[min_similarity_index]\n",
    "        lst_remain_swap = np.delete(lst_remain_swap, min_similarity_index)\n",
    "\n",
    "        fed_swap(swap_client_base, swap_client_dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_swapping():\n",
    "    sim_sparse_matrix = {\"row\": np.array(list(), dtype=\"int32\"),\n",
    "                         \"col\": np.array(list(), dtype=\"int32\"),\n",
    "                         \"val\": np.array(list())}\n",
    "\n",
    "    for row in range(len(client_selects)):\n",
    "        for col in range(row+1, len(client_selects)):\n",
    "            similarity = model_similarity(client_weights[client_selects[row]], client_weights[client_selects[col]])\n",
    "            sim_sparse_matrix[\"row\"] = np.append(sim_sparse_matrix[\"row\"], row)\n",
    "            sim_sparse_matrix[\"col\"] = np.append(sim_sparse_matrix[\"col\"], col)\n",
    "            sim_sparse_matrix[\"val\"] = np.append(sim_sparse_matrix[\"val\"], similarity)\n",
    "\n",
    "    for _ in range(int((len(client_selects)//2)*swap_percentage)):\n",
    "        min_sim_index = np.argmin(sim_sparse_matrix[\"val\"])\n",
    "        row_matrix = sim_sparse_matrix[\"row\"][min_sim_index]\n",
    "        col_matrix = sim_sparse_matrix[\"col\"][min_sim_index]\n",
    "        all_row_index_row_matrix = np.where(sim_sparse_matrix[\"row\"] == row_matrix)[0]\n",
    "        all_col_index_row_matrix = np.where(sim_sparse_matrix[\"col\"] == row_matrix)[0]\n",
    "        all_row_index_col_matrix = np.where(sim_sparse_matrix[\"row\"] == col_matrix)[0]\n",
    "        all_col_index_col_matrix = np.where(sim_sparse_matrix[\"col\"] == col_matrix)[0]\n",
    "        sim_sparse_matrix[\"val\"][all_row_index_row_matrix] = np.inf\n",
    "        sim_sparse_matrix[\"val\"][all_col_index_row_matrix] = np.inf\n",
    "        sim_sparse_matrix[\"val\"][all_row_index_col_matrix] = np.inf\n",
    "        sim_sparse_matrix[\"val\"][all_col_index_col_matrix] = np.inf\n",
    "\n",
    "        fed_swap(client_selects[row_matrix], client_selects[col_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_neural_network(dataloader, model, loss_fn):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct_items = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_items += (pred.argmax(1) == y.argmax(1)).sum().item()\n",
    "\n",
    "    avg_loss = test_loss / num_batches\n",
    "    accuracy = correct_items / data_size\n",
    "    # print(f\"Test Error: \\nAccuracy: {(accuracy*100):>0.1f}%, Loss: {avg_loss:>8f}\\n\")\n",
    "\n",
    "    return accuracy, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(dataloader, model, loss_fn, optimizer):\n",
    "    data_size = len(dataloader.dataset)\n",
    "    running_loss = 0\n",
    "\n",
    "    for batch, (x, y) in enumerate(dataloader):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(x)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        print_step = np.ceil(len(dataloader)/10)\n",
    "        if batch % print_step == 0:\n",
    "            loss_per_batch = running_loss / print_step\n",
    "            current_item = (batch+1)*len(x)\n",
    "            # print(f\"loss: {loss_per_batch:>7f}  [{current_item:>5d}/{data_size:>5d}]\")\n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clinet(dataloader, model):\n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(client_epochs):\n",
    "        train_neural_network(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_state_and_log(step):\n",
    "    # save torch_rng, if you need that\n",
    "    # torch_rng_states = [torch.get_rng_state(), torch.cuda.get_rng_state(), torch.cuda.get_rng_state_all()]\n",
    "    # torch_rng_states_ndarray = np.array(torch_rng_states, dtype=object)\n",
    "\n",
    "    dataloader_generator_state = dataloader_generator.get_state()\n",
    "    random_state = random.getstate()\n",
    "    np_random_state = np.random.get_state()\n",
    "    random_state_ndarray = np.array(random_state, dtype=object)\n",
    "    np_random_state_ndarray = np.array(np_random_state, dtype=object)\n",
    "\n",
    "    np.savez_compressed(f\"save_log/{save_file_name_pre}_{step}.npz\",\n",
    "                        client_selects=client_selects,\n",
    "                        client_weights=client_weights,\n",
    "                        global_history=global_history,\n",
    "                        dataloader_generator_state=dataloader_generator_state,\n",
    "                        random_state_ndarray=random_state_ndarray,\n",
    "                        np_random_state_ndarray=np_random_state_ndarray,\n",
    "                        # torch_rng_states_ndarray=torch_rng_states_ndarray,\n",
    "                        )\n",
    "\n",
    "    if step != 0:\n",
    "        os.remove(f\"save_log/{save_file_name_pre}_{step-1}.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_time_format(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "\n",
    "    if h:\n",
    "        return f\"{h:.0f}h-{m:.0f}m-{s:.0f}s\"\n",
    "    elif m:\n",
    "        return f\"{m:.0f}m-{s:.0f}s\"\n",
    "    else:\n",
    "        return f\"{s:.2f}s\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_log(training_time, swapping_time, step_time, step, metric_index=-1):\n",
    "    training_time = change_time_format(training_time)\n",
    "    swapping_time = change_time_format(swapping_time)\n",
    "    step_time = change_time_format(step_time)\n",
    "    print(f\"round: {step} | training_time: {training_time} | swapping_time: {swapping_time} | step_time: {step_time}\")\n",
    "\n",
    "    global is_print_eval\n",
    "    if is_print_eval:\n",
    "        is_print_eval = False\n",
    "        print(f\"round: {step} / global_acc: {start_bold}{global_history['accuracy'][metric_index]:.4%}{end_bold} / global_loss: {start_bold}{global_history['loss'][metric_index]:.4f}{end_bold}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prev_log(passed_steps):\n",
    "    if passed_steps:\n",
    "        global is_print_eval\n",
    "        metric_index = -1\n",
    "\n",
    "        for step in range(passed_steps):\n",
    "            if print_eval_each_step:\n",
    "                is_print_eval = True\n",
    "                metric_index += 1\n",
    "            else:\n",
    "                if (step % (swap_step*n_swap_bet_avg_p1) == 0):\n",
    "                    is_print_eval = True\n",
    "                    metric_index += 1\n",
    "\n",
    "            training_time = global_history[\"times\"][\"train\"][step]\n",
    "            swapping_time = global_history[\"times\"][\"swap\"][step]\n",
    "            step_time = global_history[\"times\"][\"step\"][step]\n",
    "            print_log(training_time, swapping_time, step_time, step, metric_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round: 0 | training_time: 0.82s | swapping_time: 0.00s | step_time: 9.12s\n",
      "round: 0 / global_acc: \u001b[1m20.0000%\u001b[0m / global_loss: \u001b[1m2.3015\u001b[0m\n",
      "\n",
      "round: 1 | training_time: 0.82s | swapping_time: 0.00s | step_time: 9.76s\n",
      "round: 1 / global_acc: \u001b[1m20.0000%\u001b[0m / global_loss: \u001b[1m2.3015\u001b[0m\n",
      "\n",
      "round: 2 | training_time: 0.47s | swapping_time: 0.00s | step_time: 9.28s\n",
      "round: 2 / global_acc: \u001b[1m20.0000%\u001b[0m / global_loss: \u001b[1m2.3015\u001b[0m\n",
      "\n",
      "round: 3 | training_time: 0.48s | swapping_time: 2.79s | step_time: 12.60s\n",
      "round: 3 / global_acc: \u001b[1m20.0000%\u001b[0m / global_loss: \u001b[1m2.3015\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# FedSwap_similarity Main Loop\n",
    "print_prev_log(passed_steps)\n",
    "for step in range(passed_steps, total_steps):\n",
    "    training_time_start = time.time()\n",
    "    for client in client_selects:\n",
    "        local_model = globals()[neural_network_classname]().to(device)\n",
    "        local_model.set_weights(client_weights[client])\n",
    "        train_clinet(client_dataloaders[client], local_model)\n",
    "        client_weights[client] = local_model.get_weights()\n",
    "\n",
    "        del local_model\n",
    "    \n",
    "    training_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"train\"].append(training_time)\n",
    "\n",
    "\n",
    "    swapping_time_start = time.time()\n",
    "    if (step % swap_step == 0) and (step % (swap_step*n_swap_bet_avg_p1) != 0):\n",
    "        if swap_mode == \"greedy\":\n",
    "            greedy_swapping()\n",
    "        elif swap_mode == \"best\":\n",
    "            best_swapping()\n",
    "    \n",
    "    swapping_time = time.time() - swapping_time_start\n",
    "    global_history[\"times\"][\"swap\"].append(swapping_time)\n",
    "    \n",
    "\n",
    "    if (step % (swap_step*n_swap_bet_avg_p1) == 0):\n",
    "        avg_weights = fed_avg()\n",
    "        global_model.set_weights(avg_weights) # update global model\n",
    "        select_clients_and_assign_weights(avg_weights)\n",
    "\n",
    "        is_print_eval = True\n",
    "        global_acc, global_loss = test_neural_network(test_dataloader, global_model, loss_fn)\n",
    "        global_history[\"accuracy\"].append(global_acc)\n",
    "        global_history[\"loss\"].append(global_loss)\n",
    "    else:\n",
    "        if print_eval_each_step:\n",
    "            avg_weights = fed_avg()\n",
    "            temp_global_weights = global_model.get_weights()\n",
    "            global_model.set_weights(avg_weights) # update global model\n",
    "\n",
    "            is_print_eval = True\n",
    "            global_acc, global_loss = test_neural_network(test_dataloader, global_model, loss_fn)\n",
    "            global_model.set_weights(temp_global_weights)\n",
    "            global_history[\"accuracy\"].append(global_acc)\n",
    "            global_history[\"loss\"].append(global_loss)\n",
    "    \n",
    "    step_time = time.time() - training_time_start\n",
    "    global_history[\"times\"][\"step\"].append(step_time)\n",
    "    print_log(training_time, swapping_time, step_time, step)\n",
    "    save_state_and_log(step)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"loss\"], label=\"test loss\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"accuracy\"], label=\"test accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(global_history[\"loss\"], label=\"test loss\")\n",
    "# plt.plot(global_history[\"accuracy\"], label=\"test accuracy\")\n",
    "# plt.xlabel(\"Epochs\")\n",
    "# plt.ylabel(\"Loss / Accuracy\")\n",
    "# plt.title(\"Test Data\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
